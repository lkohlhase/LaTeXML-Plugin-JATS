@article{bandit,
   author = {Auer, P. and Cesa-Bianchi, N. and Fischer, P.},
   title = {Finite-time analysis of the multiarmed bandit problem},
   journal = {Machine Learning},
   volume = {47},
   number = {2-3},
   pages = {235-256},
   annote = {ISI Document Delivery No.: 508XD
Times Cited: 342
Cited Reference Count: 12
Auer, P Cesa-Bianchi, N Fischer, P
344
Kluwer academic publ
Dordrecht
Computer Science, Artificial Intelligence},
   abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
   keywords = {bandit problems
adaptive allocation rules
finite horizon regret
policies
Computer Science},
   ISSN = {0885-6125},
   DOI = {10.1023/a:1013689704352},
   url = {<Go to ISI>://WOS:000173114300005},
   year = {2002},
   type = {Journal Article}
}

@article{funapp,
   author = {Busoniu, L. and Babuska, R. and Schutter, B. D. and Ernst, D.},
   title = {Reinforcement Learning and Dynamic Programming Using Function Approximators},
   journal = {Reinforcement Learning and Dynamic Programming Using Function Approximators},
   pages = {1-267},
   annote = {ISI Document Delivery No.: BTB21
Times Cited: 84
Cited Reference Count: 219
Busoniu, L Babuska, R Schutter, BD Ernst, D
84
Crc press-taylor & francis group
Boca raton
978-1-4398-2108-4
Automation & Control Systems; Computer Science, Artificial Intelligence},
   keywords = {markov decision-processes
actor-critic algorithms
cross-entropy
method
support vector regression
fitted-q-iteration
policy iteration

gradient methods
continuous-time
fuzzy-logic
convergence
Automation &amp; Control Systems
Computer Science},
   DOI = {10.1201/9781439821091-f},
   url = {<Go to ISI>://WOS:000286332800009},
   year = {2010},
   type = {Journal Article}
}

@article{Kae1,
   author = {Cassandra, A. R. and Kaelbling, L. P. and Littman, M. L. and Amer Assoc Artificial, Intelligence},
   title = {ACTING OPTIMALLY IN PARTIALLY OBSERVABLE STOCHASTIC DOMAINS},
   journal = {Proceedings of the Twelfth National Conference on Artificial Intelligence, Vols 1 and 2},
   pages = {1023-1028},
   annote = {ISI Document Delivery No.: BC17T
Times Cited: 60
Cited Reference Count: 0
Cassandra, ar kaelbling, lp littman, ml
12th National Conference on Artificial Intelligence
Jul 31-aug 04, 1994
Seattle, wa
Amer assoc artificial intelligence
61
M i t press
Cambridge
0-262-61102-3
Computer Science, Artificial Intelligence},
   keywords = {Computer Science},
   url = {<Go to ISI>://WOS:A1994BC17T00158},
   year = {1994},
   type = {Journal Article}
}

@article{Kae2,
   author = {Cassandra, A. R. and Kaelbling, L. P. and Littman, M. L. and Amer Assoc Artificial, Intelligence},
   title = {ACTING OPTIMALLY IN PARTIALLY OBSERVABLE STOCHASTIC DOMAINS},
   journal = {Proceedings of the Twelfth National Conference on Artificial Intelligence, Vols 1 and 2},
   pages = {1023-1028},
   annote = {ISI Document Delivery No.: BC17T
Times Cited: 60
Cited Reference Count: 0
Cassandra, ar kaelbling, lp littman, ml
12th National Conference on Artificial Intelligence
Jul 31-aug 04, 1994
Seattle, wa
Amer assoc artificial intelligence
61
M i t press
Cambridge
0-262-61102-3
Computer Science, Artificial Intelligence},
   keywords = {Computer Science},
   url = {<Go to ISI>://WOS:A1994BC17T00158},
   year = {1994},
   type = {Journal Article}
}

@article{4curr,
   author = {Dietterich, T. G.},
   title = {Machine-learning research - Four current directions},
   journal = {Ai Magazine},
   volume = {18},
   number = {4},
   pages = {97-136},
   annote = {ISI Document Delivery No.: YN301
Times Cited: 396
Cited Reference Count: 125
Dietterich, TG
480
Amer assoc artificial intell
Menlo pk
Computer Science, Artificial Intelligence},
   abstract = {Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.},
   keywords = {hidden markov-models
probabilistic networks
recognition
Computer Science},
   ISSN = {0738-4602},
   url = {<Go to ISI>://WOS:000071153200009},
   year = {1997},
   type = {Journal Article}
}

@article{MAXQ,
   author = {Dietterich, T. G.},
   title = {Hierarchical reinforcement learning with the {MAXQ} value function decomposition},
   journal = {Journal of Artificial Intelligence Research},
   volume = {13},
   pages = {227-303},
   annote = {ISI Document Delivery No.: 378EW
Times Cited: 218
Cited Reference Count: 33
Dietterich, TG
249
Ai access foundation
Marina del rey
Computer Science, Artificial Intelligence},
   abstract = {This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics-as a subroutine hierarchy-and a declarative semantics-as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than at Q learning. The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the effectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.},
   keywords = {robot
match
Computer Science},
   ISSN = {1076-9757},
   url = {<Go to ISI>://WOS:000165572900001},
   year = {2000},
   type = {Journal Article}
}

@inproceedings{Kae3,
   author = {Hsiao, K. and Kaelbling, L. P. and Lozano-Perez, T. and Ieee},
   title = {Grasping {POMDPs}},
   booktitle = {IEEE International Conference on Robotics and Automation},
   series = {Ieee International Conference on Robotics and Automation},
   address = {NEW YORK},
   publisher = {Ieee},
   pages = {4685-4692},
   annote = {ISI Document Delivery No.: BGW23
Times Cited: 25
Cited Reference Count: 29
Hsiao, Kaijen Kaelbling, Leslie Pack Lozano-Perez, Tomas
Lozano-Perez, Tomas/J-9374-2012
Lozano-Perez, Tomas/0000-0002-8657-2450
25},
   abstract = {We provide a method for planning under uncertainty for robotic manipulation by partitioning the configuration space into a set of regions that are closed under compliant motions. These regions can be treated as states in a partially observable Markov decision process (POMDP), which can be solved to yield optimal control policies under uncertainty. We demonstrate the approach on simple grasping problems, showing that it can construct highly robust, efficiently executable solutions.},
   keywords = {Automation &amp; Control Systems
Robotics},
   ISBN = {1050-4729
978-1-4244-0601-2},
   DOI = {10.1109/robot.2007.364201},
   url = {<Go to ISI>://WOS:000250915304109},
   year = {2007},
   type = {Conference Proceedings}
}

@article{Fuzzy,
   author = {Jouffe, L.},
   title = {Fuzzy inference system learning by reinforcement methods},
   journal = {Ieee Transactions on Systems Man and Cybernetics Part C-Applications and Reviews},
   volume = {28},
   number = {3},
   pages = {338-355},
   annote = {ISI Document Delivery No.: 104JF
Times Cited: 133
Cited Reference Count: 63
Jouffe, L
151
Ieee-inst electrical electronics engineers inc
Piscataway
Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Computer Science, Interdisciplinary Applications},
   abstract = {Fuzzy Actor-Critic Learning (FACL) and Fuzzy Q-Learning (FQL) are reinforcement learning methods based on Dynamic Programming (DP) principles. In this paper, they are used to tune online the conclusion part of Fuzzy Inference Systems (FIS), The only information available for learning is the system feedback, which describes in terms of reward and punishment the task the fuzzy agent has to realize. At each time step, the agent receives a reinforcement signal according to the last action it has performed in the previous state. The problem involves optimizing not only the direct reinforcement, but also the total amount of reinforcements the agent can receive in the future. To illustrate the use of these two learning methods, we first applied them to a problem that involves finding a fuzzy controller to drive a boat from one bank to another, across a river with a strong nonlinear current. Then, we used the well-known Cart-Pole Balancing and Mountain-Car problems to be able to compare our methods to other reinforcement learning methods and focus on important characteristic aspects of FACL and FQL, We found that the genericity of our methods allows us to learn every kind of reinforcement learning problem (continuous states, discrete/continuous actions, various type of reinforcement functions). The experimental studies also show the superiority of these methods with respect to the other related methods we can find in the literature.},
   keywords = {Dynamic Programming (DP)
fuzzy logic
learning
Markovian Decision
Problem (MDP)
reinforcement
logic controller
convergence
algorithms
Computer Science},
   ISSN = {1094-6977},
   DOI = {10.1109/5326.704563},
   url = {<Go to ISI>://WOS:000075010400004},
   year = {1998},
   type = {Journal Article}
}

@article{Kae4,
   author = {Kaelbling, L. P. and Littman, M. L. and Moore, A. W.},
   title = {Reinforcement learning: A survey},
   journal = {Journal of Artificial Intelligence Research},
   volume = {4},
   pages = {237-285},
   annote = {ISI Document Delivery No.: UJ312
Times Cited: 1377
Cited Reference Count: 123
Kaelbling, LP Littman, ML Moore, AW
1471
Ai access foundation
Marina del rey
Computer Science, Artificial Intelligence},
   abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ''reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
   keywords = {algorithms
Computer Science},
   ISSN = {1076-9757},
   url = {<Go to ISI>://WOS:A1996UJ31200001},
   year = {1996},
   type = {Journal Article}
}

@article{adp1,
   author = {Lewis, F. L. and Vrabie, D.},
   title = {Reinforcement Learning and Adaptive Dynamic Programming for Feedback Control},
   journal = {Ieee Circuits and Systems Magazine},
   volume = {9},
   number = {3},
   pages = {32-50},
   annote = {ISI Document Delivery No.: 502CW
Times Cited: 98
Cited Reference Count: 91
Lewis, Frank L. Vrabie, Draguna
Nsf [eccs-0801330]; aro [w91nf-05-1-0314]
We acknowledge the support of NSF Grant ECCS-0801330 and ARO grant W91NF-05-1-0314.
106
Ieee-inst electrical electronics engineers inc
Piscataway
Engineering, Electrical & Electronic},
   abstract = {Living organisms learn by acting on their environment, observing the resulting reward stimulus, and adjusting their actions accordingly to improve the reward. This action-based or Reinforcement Learning can capture notions of optimal behavior occurring in natural systems. We describe mathematical formulations for Reinforcement Learning and a practical implementation method known as Adaptive Dynamic Programming. These give us insight into the design of controllers for man-made engineered systems that both learn and exhibit optimal behavior.},
   keywords = {control-systems
neural-network
iterative technique
nonlinear-systems

continuous-time
optimization
convergence
equation
reward

neurocontrol
Engineering},
   ISSN = {1531-636X},
   DOI = {10.1109/mcas.2009.933854},
   url = {<Go to ISI>://WOS:000270433000004},
   year = {2009},
   type = {Journal Article}
}

@article{Conv1,
   author = {Singh, S. and Jaakkola, T. and Littman, M. L. and Szepesvari, C.},
   title = {Convergence results for single-step on-policy reinforcement-learning algorithms},
   journal = {Machine Learning},
   volume = {38},
   number = {3},
   pages = {287-308},
   annote = {ISI Document Delivery No.: 283CZ
Times Cited: 116
Cited Reference Count: 37
Singh, S Jaakkola, T Littman, ML Szepesvari, C
130
Kluwer academic publ
Dordrecht
Computer Science, Artificial Intelligence},
   abstract = {An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate exploration from learning and therefore must confront the exploration problem directly. We prove convergence results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also provide examples of exploration strategies that can be followed during learning that result in convergence to both optimal values and optimal policies.},
   keywords = {reinforcement-learning
on-policy
convergence
Markov decision
processes
Computer Science},
   ISSN = {0885-6125},
   DOI = {10.1023/a:1007678930559},
   url = {<Go to ISI>://WOS:000085259700003},
   year = {2000},
   type = {Journal Article}
}

@article{Pair,
   author = {Thrun, S.},
   title = {Probabilistic algorithms in robotics},
   journal = {Ai Magazine},
   volume = {21},
   number = {4},
   pages = {93-109},
   annote = {ISI Document Delivery No.: 385HH
Times Cited: 143
Cited Reference Count: 96
Thrun, S
153
Amer assoc artificial intell
Menlo pk
Computer Science, Artificial Intelligence},
   abstract = {This article describes a methodology for programming robots known as probabilistic robotics. The probabilistic paradigm pays tribute to the inherent uncertainty in robot perception, relying on explicit representations of uncertainty when determining what to do. This article surveys some of the progress in the field, using in-depth examples to illustrate some of the nuts and bolts of the basic approach. My central conjecture is that the probabilistic approach to robotics scales better to complex real-world applications than approaches that ignore a robot's uncertainty.},
   keywords = {observable markov-processes
mobile robots
dynamic environments

localization
navigation
horizon
Computer Science},
   ISSN = {0738-4602},
   url = {<Go to ISI>://WOS:000165997700008},
   year = {2000},
   type = {Journal Article}
}

@article{Qlearn,
   author = {Watkins, C. and Dayan, P.},
   title = {Q-LEARNING},
   journal = {Machine Learning},
   volume = {8},
   number = {3-4},
   pages = {279-292},
   annote = {ISI Document Delivery No.: HV723
Times Cited: 1583
Cited Reference Count: 14
Watkins, cjch dayan, p
Watkins, Christopher/E-5811-2013
Watkins, Christopher/0000-0001-9020-4530
1705
Kluwer academic publ
Dordrecht
Computer Science, Artificial Intelligence},
   abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem for Q-learning based on that outlined in Watkins (1989). We show that Q-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
   keywords = {q-learning
reinforcement learning
temporal differences
asynchronous
dynamic programming
Computer Science},
   ISSN = {0885-6125},
   DOI = {10.1023/a:1022676722315},
   url = {<Go to ISI>://WOS:A1992HV72300004},
   year = {1992},
   type = {Journal Article}
}
@phdthesis{Anca,
  author = {A. Dragan},
  title = {Planning With Observable Operator Models},
  school = {Jacobs University Bremen},
  year = {2009},
  type = {Bachelor thesis},
  address = {},
  month = {June}
}
@article{tuto1,
  author = {H. Jaeger},
  title = {Observable operator models of stochastic processes: a tutorial},
  journal = {GMD Report},
  volume = {42},
	year={1998},
	adress={St. Augustin}
}
@article{OOM1, 
  author ={H. Jaeger and M. Zhao and K. Kretzschmar and T. Oberstein and D. Popovici and A. Kolling},
	title ={Learning observable operator models via the {ES}-algorithm},
	journal={New Directions in Statistical Signal Processing: from Systems to Brain},
	publisher={MIT Press},
	pages={417-464},
	adress={Cambridge,MA},
	year={2006}
}
@article{Elev,
   author = {Crites, R. H. and Barto, A. G.},
   title = {Improving elevator performance using reinforcement learning},
   journal = {Advances in Neural Information Processing Systems 8: Proceedings of the 1995 Conference},
   volume = {8},
   pages = {1017-1023},
   annote = {ISI Document Delivery No.: BG45M
Times Cited: 105
Cited Reference Count: 0
Crites, RH Barto, AG
Touretzky, DS Mozer, MC Hasselmo, ME
9th Annual Conference on Neural Information Processing Systems (NIPS)
Nov 27-30, 1995
Denver, co
106
M i t press
Cambridge
0-262-20107-0
Computer Science, Information Systems; Neurosciences},
   keywords = {Computer Science
Neurosciences &amp; Neurology},
   ISSN = {1049-5258},
   url = {<Go to ISI>://WOS:A1996BG45M00143},
   year = {1996},
   type = {Journal Article}
}
@article{MDP,
   author = {Bellman, R.},
   title = {THE THEORY OF DYNAMIC PROGRAMMING},
   journal = {Bulletin of the American Mathematical Society},
   volume = {60},
   number = {6},
   pages = {503-515},
   annote = {ISI Document Delivery No.: XS482
Times Cited: 101
Cited Reference Count: 50
Bellman, r
101
Amer mathematical soc
Providence
Mathematics},
   keywords = {Mathematics},
   ISSN = {0273-0979},
   DOI = {10.1090/s0002-9904-1954-09848-8},
   url = {<Go to ISI>://WOS:A1954XS48200001},
   year = {1954},
   type = {Journal Article}
}

@article{MDPcomplex,
   author = {Papadimitriou, C. H. and Tsitsiklis, J. N.},
   title = {THE COMPLEXITY OF MARKOV DECISION-PROCESSES},
   journal = {Mathematics of Operations Research},
   volume = {12},
   number = {3},
   pages = {441-450},
   annote = {ISI Document Delivery No.: K0230
Times Cited: 230
Cited Reference Count: 17
Papadimitriou, ch tsitsiklis, jn
232
Inst operations research management sciences
Linthicum hts
Operations Research and Management Science; Mathematics, Applied},
   keywords = {Operations Research &amp; Management Science
Mathematics},
   ISSN = {0364-765X},
   DOI = {10.1287/moor.12.3.441},
   url = {<Go to ISI>://WOS:A1987K023000004},
   year = {1987},
   type = {Journal Article}
}
