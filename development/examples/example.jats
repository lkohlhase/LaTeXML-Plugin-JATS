<?xml version="1.0"?>
<article>
  <front>
    <article-meta>
      <title-group>
        <article-title>Nonparametic Bayesian Double Articulation Analyzerfor Direct Language Acquisition from Continuous Speech Signals</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Taniguchi</surname>
            <given-names>Tadahiro 
			</given-names>
          </name>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Nagasaka</surname>
            <given-names>Shogo 
			</given-names>
          </name>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Nakashima</surname>
            <given-names>Ryo 
			</given-names>
          </name>
        </contrib>
      </contrib-group>
      <abstract>
        <p>Human infants can discover words directly from unsegmented speech signals without any explicitly labeled data. In this paper, we develop a novel machine learning method called nonparametric Bayesian double articulation analyzer (NPB-DAA) that can directly acquire language and acoustic models from observed continuous speech signals. For this purpose, we propose an integrative generative model that combines a language model and an acoustic model into a single generative model called the “hierarchical Dirichlet process hidden language model” (HDP-HLM). The HDP-HLM is obtained by extending the hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. An inference procedure for the HDP-HLM is derived using the blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure enables the simultaneous and direct inference of language and acoustic models from continuous speech signals. Based on the HDP-HLM and its inference procedure, we developed a novel double articulation analyzer.
By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, the method can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner.
The novel unsupervised double articulation analyzer is called NPB-DAA.
The NPB-DAA can automatically estimate double articulation structure embedded in speech signals. We also carried out two evaluation experiments using synthetic data and actual human continuous speech signals representing Japanese vowel sequences. In the word acquisition and phoneme categorization tasks, the NPB-DAA outperformed a conventional double articulation analyzer (DAA) and baseline automatic speech recognition system whose acoustic model was trained in a supervised manner.</p>
      </abstract>
      <ERROR xmlns="http://dlmf.nist.gov/LaTeXML" class="undefined">{IEEEkeywords}</ERROR>
      <!-- An error in the conversion from LaTeX to XML has occurred here. -->
      <ERROR xmlns="http://dlmf.nist.gov/LaTeXML" class="undefined">\IEEEpeerreviewmaketitle</ERROR>
      <!-- An error in the conversion from LaTeX to XML has occurred here. -->
    </article-meta>
  </front>
  <body>
    <ERROR xmlns="http://dlmf.nist.gov/LaTeXML" class="undefined">{IEEEkeywords}</ERROR>
    <!-- An error in the conversion from LaTeX to XML has occurred here. -->
    <p id="p1">Language acquisition, child development, Bayesian nonparametrics, latent variable model</p>
    <ERROR xmlns="http://dlmf.nist.gov/LaTeXML" class="undefined">\IEEEpeerreviewmaketitle</ERROR>
    <!-- An error in the conversion from LaTeX to XML has occurred here. -->
    <sec id="S1">
      <title>INTRODUCTION</title>
      <ERROR xmlns="http://dlmf.nist.gov/LaTeXML" class="undefined">\IEEEPARstart</ERROR>
      <!-- An error in the conversion from LaTeX to XML has occurred here. -->
      <p id="S1.p1">INFANTS must solve the word segmentation problem in order to acquire language from the continuous speech signals to which they are exposed. The word segmentation problem is that of identifying word boundaries in continuous speech. If the speech signals are given to infants as isolated words, the task is easy for them. However, it has been known that a relatively small number of infant-directed utterances consist of an isolated word <xref ref-type="bibr" rid="Aslin1996">[Aslin1996]</xref>.
If infants had knowledge about words and phonemes innately, the problem could be solved relatively easily.
However, the fact that each language has different lists of phonemes and words clearly shows that infants have to acquire them through developmental processes.</p>
      <p id="S1.p2">From the viewpoint of statistical learning, the learning problem, i.e., direct language acquisition from continuous speech signals, is very difficult because infants do not have access to the truth labels of speech recognition results. In other words, the language acquisition process must be completely unsupervised, in contrast with most current automatic speech recognition (ASR) systems.</p>
      <p id="S1.p3">Most modern ASR systems have a language model that represents knowledge about words and their distributional probabilities as well as an acoustic model that represents knowledge about phonemes and their acoustic features. Both are usually trained using large transcribed speech datasets and linguistic corpora through supervised learning. However, infants do not have access to such explicitly labeled datasets.
They have to acquire both language and acoustic models from raw acoustic speech signals in an unsupervised manner.</p>
      <p id="S1.p4">The question about what kind of cues human infants utilize to discover words from continuous speech signals arises.
Saffran et al. listed three types of cues for word segmentation: prosodic, distributional, and co-occurrence <xref ref-type="bibr" rid="Saffran1996a">[Saffran1996a]</xref>.
Prosodic cues rely on acoustic information, such as post-utterance pauses, stressed syllables, and acoustically distinctive final syllables.
Distributional cues represent the statistical relationships between neighboring speech sounds.
Co-occurrence cues are used by children to learn words by detecting which sounds co-occurs with entities in the environment.
Although many researchers had considered the distributional cues to be too complex for infants to use,
Saffran reported that word segmentation from fluent speech can be accomplished by 8-month-old infants based on solely on distributional cues <xref ref-type="bibr" rid="Saffran1996">[Saffran1996]</xref>. It is also reported that the distributional cues seem to be used
by infants by the age of 7 months, which is earlier than most other cues <xref ref-type="bibr" rid="Thiessen2003">[Thiessen2003]</xref>.
These results imply that infants have a fundamental mechanism that can estimate word segments using distributional cues. In addition to this fundamental segmentation mechanism using distributional cues, the prosodic and co-occurrence cues are believed to help the word segmentation task only as
supplemental cues. In addition, from the viewpoint of phonemic category acquisition, distributional patterns of sounds have been considered to provide infants with clues about the phonemic structure of a language <xref ref-type="bibr" rid="Kuhl2007">[Kuhl2007]</xref>.</p>
      <p id="S1.p5">Based on these findings, in this paper, we focus on distributional cues.
We explore the fundamental computational mechanism that can discover words from speech signals using only distributional cues, and develop an unsupervised machine learning method which can discover phonemes and words directly from unsegmented speech signals</p>
      <p id="S1.p6">In this paper, we propose an unsupervised learning method called the nonparametric Bayesian double articulation analyzer (NPB-DAA) which can automatically estimate double articulation structure, i.e., hierarchically organized latent words and phonemes, embedded in speech signals. We proposed it as a computationally feasible explanation for the simultaneous acquisition of language and acoustic models. To develop the NPB-DAA, we newly introduce probabilistic generative model called the hierarchical Dirichlet process hidden language model (HDP-HLM) and its inference algorithm.</p>
      <p id="S1.p7">The remainder of this paper is organized as follows.
Section <xref rid="LABEL:sec2">2</xref> describes the background of the proposed method.
Section <xref rid="LABEL:sec3">3</xref> presents the HDP-HLM by extending hierarchical Dirichlet process-hidden semi-Markov model (HDP-HSMM) proposed by Johnson et al. <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref>. The HDP-HLM is an probabilistic generative model that integrates acoustic and language models for continuous speech signals.
Section <xref rid="LABEL:sec4">4</xref> describes the inference procedure of HDP-HLM, and our proposed NPB-DAA. Sections <xref rid="LABEL:sec5">5</xref> and <xref rid="LABEL:sec6">6</xref> evaluate the effectiveness of the proposed method using synthetic data and actual sequential vowel speech signals. Section <xref rid="LABEL:sec7">7</xref> concludes this paper.</p>
    </sec>
    <sec id="S2">
      <title>Background</title>
      <sec id="S2.SS1">
        <title>Word segmentation using distributional cues in transcribed data</title>
        <p id="S2.SS1.p1">With respect to statistical computational models, many kinds of unsupervised machine learning methods for word segmentation have been proposed in the last two decades <xref ref-type="bibr" rid="Brent1999">[Brent1999, Venkataraman2001, Goldwater2008, Goldwater2009, Mochihashi2009, Johnson2009, Chen2014, Magistry2012, Sakti2011]</xref>.
Brent proposed model-based dynamic programming 1 (MBDP-1) for recovering deleted word boundaries in a natural-language text <xref ref-type="bibr" rid="Brent1999">[Brent1999]</xref>. The MBDP-1 presumes that there is an information source generating the text explicitly and segments the target text so as to maximize the text’s probability. Venkataraman proposed a statistical model for segmentation and word discovery from phoneme sequences by improving Brent’s algorithm <xref ref-type="bibr" rid="Venkataraman2001">[Venkataraman2001]</xref>.
</p>
        <p id="S2.SS1.p2">Recently, Bayesian nonparametrics, including the hierarchical Dirichlet process and hierarchical Pitman-Yor process, have enabled more sophisticated methods for word segmentation. These models have fully Bayesian generative models and make it possible to calculate the appropriately smoothed n-gram probability for a word that has a long context. Theoretically, they can treat an infinite number of possible words.
Goldwater proposed an HDP-based word segmentation method and showed that taking context into account is important for statistical word segmentation <xref ref-type="bibr" rid="Goldwater2008">[Goldwater2008, Goldwater2009]</xref>.
Mochihashi et al. proposed a nested Pitman-Yor language model (NPYLM), in which a letter n-gram model based on a hierarchical Pitman-Yor language model is embedded in the word n-gram model. They also developed the forward filtering backward sampling procedure to achieve efficient blocked Gibbs sampling and hence infer word boundaries <xref ref-type="bibr" rid="Mochihashi2009">[Mochihashi2009]</xref>.</p>
        <p id="S2.SS1.p3">However, all of the above mentioned word segmentation methods presume that transcribed phoneme sequences or text data without any recognition errors can be obtained by the learning system.
However, in practice, before acquiring a language model containing an inventory of words, a learning system, i.e., an infant, has to recognize speech signals without any knowledge of words, only with the knowledge of phonemes and/or syllables in an acoustic model. In such a recognition task, the phoneme recognition error rate inevitably becomes high.
To overcome this problem, several researchers have proposed word discovery methods utilizing co-occurrence cues.</p>
      </sec>
      <sec id="S2.SS2">
        <title>Lexical acquisition using co-occurrence cues</title>
        <p id="S2.SS2.p1">Roy et al. ambitiously implemented a computational model that enables a robot to autonomously discover words from raw multimodal sensory input <xref ref-type="bibr" rid="Roy2002a">[Roy2002a]</xref>. Their results were imperfect compared with recent state-of-art results. However, their results showed it was possible to develop cognitive models that can process raw sensor data and acquire a lexicon without the need for human transcription or labeling.</p>
        <p id="S2.SS2.p2">Iwahashi et al. implemented an interactive learning method for a robot to acquire spoken words through human-robot interaction using audio-visual interfaces <xref ref-type="bibr" rid="Iwahashi2008">[Iwahashi2008]</xref>. Their learning process was carried out on-line, incrementally, actively, and in an unsupervised manner.
Iwahashi et al. also proposed a method that enables a robot to learn linguistic knowledge through human-robot communication in an unsupervised manner <xref ref-type="bibr" rid="Iwahashi2003">[Iwahashi2003]</xref>.
The model combines speech, visual, and behavioral information in a probabilistic framework.
Though its performance was still limited, the model is considered to be a more sophisticated model than that proposed in Roy et al.’s previous study from the viewpoint of statistical machine learning <xref ref-type="bibr" rid="Roy2002a">[Roy2002a]</xref>.
On the basis of this work, Iwahashi et al. developed an integrated online machine learning system combining speech, visual, and tactile information obtained through interaction. It enabled robots to learn beliefs regarding speech units, words, the concepts of objects, motions, grammar, and pragmatic and communicative capabilities <xref ref-type="bibr" rid="Iwahashi2010">[Iwahashi2010]</xref>.
They called the system LCore.</p>
        <p id="S2.SS2.p3">Araki et al. built a robot that formed object categories and acquired their names by combining a multimodal latent Dirichlet allocation (MLDA) and the NPYLM <xref ref-type="bibr" rid="Araki2012">[Araki2012]</xref>. They showed that the iterative learning of MLDA and NPYLM increases word segmentation performance by using distributional cues and co-occurrence cues simultaneously, but they reported that the prediction accuracy decreases as the phoneme recognition error rate increases.
To overcome this problem, Nakamura et al. integrated statistical models for word segmentation and multimodal categorization. They showed that a robot can autonomously form object categories and related words from continuous speech signals and continuous visual, auditory, and haptic information by updating its language and categorization models iteratively <xref ref-type="bibr" rid="Nakamura2014">[Nakamura2014]</xref>.</p>
        <p id="S2.SS2.p4">Not only object information, but also place information can be used as co-occurrence cues.
Taguchi et al. proposed a method for the unsupervised learning of place-names from information pairs that consist of spoken utterances and the mobile robot’s estimated current location without any prior linguistic knowledge other than a phoneme acoustic model <xref ref-type="bibr" rid="Taguchi2011">[Taguchi2011]</xref>. They optimized a word list using a model selection method based on description length criterion.
Taniguchi et al. combined NPYLM and Monte Carlo localization (MCL) methods and developed simultaneous localization and word discovery methods for a robot. Their model updates language model and speech recognition results iteratively by referring to the robot’s location information, as estimated by MCL <xref ref-type="bibr" rid="Akira2015">[Akira2015]</xref>.
</p>
      </sec>
      <sec id="S2.SS3">
        <title>Word segmentation using distributional cues in noisy input</title>
        <p id="S2.SS3.p1">As described above, it becomes clear that using co-occurrence cues can mitigate the ill effects of phoneme recognition errors in a word discovery task. However, whether or not the word discovery task can be achieved solely from raw speech signals is still an open question.
Neubig et al. extended the unsupervised morphological analyzer proposed by Mochihashi et al. and enabled it to analyze phoneme lattices <xref ref-type="bibr" rid="Neubig2012">[Neubig2012]</xref>. Heymann et al. modified Neubig et al.’s algorithm and proposed a suboptimal two-stage algorithm <xref ref-type="bibr" rid="Heymann2013">[Heymann2013]</xref>. Heymann et al. reported that their proposed method outperformed the original method in an experiment that used lattice input generated artificially from text input. In addition, they used the discovered language model for phoneme recognition in an iterative manner and reported that recognition performance was improved <xref ref-type="bibr" rid="Heymann2014">[Heymann2014]</xref>.
Elsner et al. proposed a computational model that jointly performs word segmentation and learns an explicit model of phonetic variation <xref ref-type="bibr" rid="Elsner2013">[Elsner2013]</xref>. However, they did not start with acoustic sound, but with dictated noisy text, i.e., recognized phoneme sequences with errors. Their model does not include acoustic model learning.</p>
        <fig id="S2.F1">
          <caption>
            <p>Overview of unsupervised learning of language and acoustic models through human-robot interaction, and the generative process of speech signal assumed in the DAA</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="daa3.eps" id="S2.F1.g1"/>
        </fig>
        <p id="S2.SS3.p2">They showed that the ill effect of phoneme recognition errors can be mitigated to some extent by using distributional information more appropriately.
However, all of these methods, except for Iwahashi et al., used an acoustic model previously trained in a supervised manner. Therefore, these models are insufficient as a constructive model for language acquisition from raw speech signals. Hence, the unsupervised learning of an acoustic model is also an important problem.</p>
      </sec>
      <sec id="S2.SS4">
        <title>Unsupervised learning of an acoustic model</title>
        <p id="S2.SS4.p1">In contrast with the word segmentation task, the acquisition of an acoustic model is basically a categorization task of the feature vectors transformed from continuous speech signals.
Mixture models, including hidden Markov models (HMMs) and Gaussian mixture models, have been used to model phoneme category acquisition. For example, Lake et al. used an online mixture estimation model for vowel category learning <xref ref-type="bibr" rid="Lake2009">[Lake2009]</xref>.
However, the phoneme acquisition has proven to be complex categorization task in a feature space.
The distribution of the feature vectors of each phoneme overlap with each other, and the actual sound of the phoneme depends on its context.
Feldman et al. pointed out that feedback information from segmented words is important for phonetic category acquisition. They demonstrated this effect through simulations using Bayesian models <xref ref-type="bibr" rid="Feldman2013">[Feldman2013]</xref>.</p>
        <p id="S2.SS4.p2">Lee et al. proposed a hierarchical Bayesian model that can discover a proper set of sub-word units and an acoustic model in an unsupervised manner <xref ref-type="bibr" rid="Lee2012">[Lee2012]</xref>. However, their model did not estimate the language model.
Lee et al. also proposed a hierarchical Bayesian model simultaneously discovering the phonetic inventory and the Letter-to-Sound mapping rules on the basis of transcribed data only <xref ref-type="bibr" rid="Lee2013a">[Lee2013a]</xref>.
The method is not a completely unsupervised learning method from raw speech signals, but does automatically determine relations between sounds and transcribed alphabets and forms an acoustic model in an unsupervised manner.</p>
        <p id="S2.SS4.p3">There have been several studies about the simultaneous unsupervised learning of acoustic and language models.
However, a very small number of statistical learning methods that can simultaneously acquire integrated acoustic and language models have been proposed.
Brandl et al. attempted to develop an unsupervised learning method that enables a robot to simultaneously obtain phonemes, syllables, and words from acoustic speech <xref ref-type="bibr" rid="Brandl2008">[Brandl2008]</xref>. They did not successfully build such a system, but reported their preliminary results.
Walter et al. proposed a word discovery method that uses an HMM-based method for finding acoustic unit descriptors in parallel with a dynamic time warping technique for finding word segments <xref ref-type="bibr" rid="Walter2013">[Walter2013]</xref>. However, their model is still heuristic from the viewpoint of probabilistic computational models.
As Feldman et al. pointed out, word segmentation and phonetic category acquisition are undoubtedly mutually dependent. Therefore, a theoretically integrated probabilistic generative model for the simultaneous acquisition of language and acoustic models is desirable.
To develop such an integrated theoretical model, the authors introduced the general concept of double articulation analysis.</p>
      </sec>
      <sec id="S2.SS5">
        <title>Double articulation analysis</title>
        <p id="S2.SS5.p1">From a general point of view, unsupervised word discovery from raw speech signals is regarded as a double articulation analysis of the time series data representing a speech signal.
The double articulation structure is a well-known two-layer hierarchical structure, i.e., a word sequence is generated from a language model, a word is a sequence of phonemes, and each phoneme outputs observation data during the period it persists. The word discovery problem becomes a general problem about analyzing the time series data that potentially have a double articulation structure by estimating the latent acoustic model as well as the latent language model.</p>
        <p id="S2.SS5.p2">Taniguchi et al. proposed a double articulation analyzer (DAA) by combining the sticky HDP-HMM and the NPYLM <xref ref-type="bibr" rid="Taniguchi2011">[Taniguchi2011]</xref>. The sticky HDP-HMM proposed by Fox et al. is an nonparametric Bayesian extension of HMM <xref ref-type="bibr" rid="Fox2009">[Fox2009]</xref>.
They applied the DAA to human motion data to extract unit motion from unsegmented human motion data.
However, they simply used the two nonparametric Bayesian methods sequentially. They did not integrated the two models into a single generative model. Therefore, if there are many recognition or categorization errors in the result of the first latent letter recognition process, i.e., segmentation process by the sticky HDP-HMM, the performance of the subsequent process, i.e., unsupervised chunking by the NPYLM, deteriorates.
In the terminology of a DAA, a latent letter and a latent word basically correspond to a phoneme and a word in speech signals, respectively.
In this paper, we call this method “conventional DAA” in order to differentiate it from the DAA newly proposed in this paper, i.e., NPB-DAA. Conventional DAA has been successfully applied to human motion data and driving behavior data, which were also considered to potentially have a double articulation structure. Conventional DAA has been used for various purposes, e.g., segmentation <xref ref-type="bibr" rid="takenaka12iros">[takenaka12iros]</xref>, prediction <xref ref-type="bibr" rid="taniguchi12iv">[taniguchi12iv, IEEE-ITS]</xref>, data mining <xref ref-type="bibr" rid="genki12sii">[genki12sii]</xref>, topic modeling <xref ref-type="bibr" rid="bando13iv">[bando13iv, bando13iros]</xref>, and video summarization <xref ref-type="bibr" rid="takenaka12mm">[takenaka12mm]</xref>.
Conventional DAA owes its successful result with respect to driving behavior data to the fact that driving behavior data were continuous and smooth compared with raw speech signals. For a driving letter, which corresponds to a phoneme in continuous speech signals, the recognition error rate was still low.
However, it is expected that a straightforward application of the conventional DAA to raw speech signals will inevitably turn out badly.</p>
        <p id="S2.SS5.p3">Therefore, based on the background mentioned above, in this paper, we propose an integrated probabilistic generative model, HDP-HLM, representing a latent double articulation structure that contains both a language model and an acoustic model. By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, we can analyze latent double articulation structure of the data in an unsupervised manner. A novel double articulation analyzer is developed on the basis of the HDP-HLM and its inference algorithm.
This HDP-HLM-based double articulation analysis method is called NPB-DAA.</p>
      </sec>
    </sec>
    <sec id="S3">
      <title>Generative model</title>
      <p id="S3.p1">In this section, we propose a novel generative model, the HDP-HLM, for time series data that potentially has a double articulation structure, by extending HDP-HSMM <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref>.
As indicated in its name, HDP-HLM latently contains a language model. In contrast with the conventional case where a latent state transits to the next state on the basis of a Markov process in the HDP-HMM, a latent word in the HDP-HLM transits to the next latent word on the basis of a language model.
An illustrative overview of the proposed method and the target task are shown in Fig. <xref rid="LABEL:fig:daa">1</xref>.
We can naturally derive an inference procedure for the HDP-HLM based on the blocked Gibbs sampler. First, we briefly describe the HDP-HSMM. We then describe the HDP-HLM.</p>
      <sec id="S3.SS1">
        <title>HDP-HSMM</title>
        <p id="S3.SS1.p1">HDP-HSMM is a nonparametric Bayesian extension of the conventional hidden semi-Markov model (HSMM) <xref ref-type="bibr" rid="johnson2013">[johnson2013, HSMM]</xref>. Unlike HDP-HMM, which is an nonparametric Bayesian extension of conventional hidden Markov model (HMM) <xref ref-type="bibr" rid="Fox2009">[Fox2009, teh2006]</xref>, the HDP-HSMM explicitly models the duration time of a hidden state.
A graphical model of the HDP-HSMM is shown in Fig. <xref rid="LABEL:fig:graphical_model_of_hdp-hmm">2</xref>.</p>
        <fig id="S3.F2">
          <caption>
            <p>Model of the HDP-HSMM <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref></p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="gm_hdp_hmm_2.eps" id="S3.F2.g1"/>
        </fig>
        <p id="S3.SS1.p2">The generative process of the HDP-HSMM is described as follows.</p>
        <p>
          <disp-formula-group id="S7.EGx1">
            <disp-formula id="S3.E1"/>
            <disp-formula id="S3.E2"/>
            <disp-formula id="S3.E3"/>
            <disp-formula id="S3.E4"/>
            <disp-formula id="S3.E5"/>
            <disp-formula id="S3.E6"/>
            <disp-formula id="S3.E7"/>
            <disp-formula id="S3.E8"/>
            <disp-formula id="S3.E9"/>
          </disp-formula-group>
        </p>
        <p>where <inline-formula id="S3.SS1.p2.m1"/> and <inline-formula id="S3.SS1.p2.m2"/> represent the stick breaking process and Dirichlet process, respectively <xref ref-type="bibr" rid="Sethuraman1994">[Sethuraman1994, teh2006]</xref>.
The parameters <inline-formula id="S3.SS1.p2.m3"/> and <inline-formula id="S3.SS1.p2.m4"/> are hyperparameters of the <inline-formula id="S3.SS1.p2.m5"/>, <inline-formula id="S3.SS1.p2.m6"/> is a global transition probability that becomes the base measure of the transition probability distributions related to each super state, and <inline-formula id="S3.SS1.p2.m7"/> is a transition probability distribution related to the <inline-formula id="S3.SS1.p2.m8"/>-th super state. Additionally, <inline-formula id="S3.SS1.p2.m9"/> and <inline-formula id="S3.SS1.p2.m10"/> are base measures for emission distribution and duration distribution.</p>
        <p id="S3.SS1.p3">In contrast with the case where HMM assumes that a hidden state <inline-formula id="S3.SS1.p3.m1"/> transits to the next hidden state <inline-formula id="S3.SS1.p3.m2"/> according to a Markov process, the hidden semi-Markov Model (HSMM) assumes that a hidden super state <inline-formula id="S3.SS1.p3.m3"/> transits to next hidden super state <inline-formula id="S3.SS1.p3.m4"/> after a probabilistically determined duration time <inline-formula id="S3.SS1.p3.m5"/>, which is sampled from a duration distribution <inline-formula id="S3.SS1.p3.m6"/>.
The super state <inline-formula id="S3.SS1.p3.m7"/> is sampled from a categorical distribution <inline-formula id="S3.SS1.p3.m8"/> related to the previous super state <inline-formula id="S3.SS1.p3.m9"/>.
When the super state <inline-formula id="S3.SS1.p3.m10"/> and duration time <inline-formula id="S3.SS1.p3.m11"/> are sampled, a sequence of hidden states <inline-formula id="S3.SS1.p3.m12"/> are determined to be <inline-formula id="S3.SS1.p3.m13"/>. An observation datum <inline-formula id="S3.SS1.p3.m14"/> at time <inline-formula id="S3.SS1.p3.m15"/> is assumed to be drawn from an emission distribution <inline-formula id="S3.SS1.p3.m16"/> whose parameter is <inline-formula id="S3.SS1.p3.m17"/>.
Observation data <inline-formula id="S3.SS1.p3.m18"/> are generated by <inline-formula id="S3.SS1.p3.m19"/> for <inline-formula id="S3.SS1.p3.m20"/> steps.</p>
        <p id="S3.SS1.p4">An efficient sampling inference procedure based on the backward filtering forward sampling technique was proposed for
constructing a blocked Gibbs sampler <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref>. A similar algorithm was proposed for HDP-HMM by Fox et al. <xref ref-type="bibr" rid="Fox2009">[Fox2009]</xref>. The algorithm is derived from a weak-limit approximation of the number of hidden super states. The computational cost of the message passing algorithm can be reduced to <inline-formula id="S3.SS1.p4.m1"/>, where <inline-formula id="S3.SS1.p4.m2"/> is the length of the observed data, <inline-formula id="S3.SS1.p4.m3"/> is the state cardinality, and <inline-formula id="S3.SS1.p4.m4"/> is the maximal duration of a super state for truncation. The order is almost the same as that of the backward filtering forward sampling algorithm for the HDP-HMM, except for the constant factor <inline-formula id="S3.SS1.p4.m5"/>.</p>
      </sec>
      <sec id="S3.SS2">
        <title>HDP-HLM</title>
        <p id="S3.SS2.p1">The generative model for time series data that potentially have a double articulation structure can be obtained by extending the HDP-HSMM.
A graphical model of the proposed HDP-HLM is shown in Fig. <xref rid="LABEL:fig:graphical_model">3</xref>.
</p>
        <fig id="S3.F3">
          <caption>
            <p>Model of the proposed HDP-HLM</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="graphical_model_daa_2.eps" id="S3.F3.g1"/>
        </fig>
        <p id="S3.SS2.p2">In the generative model of HDP-HLM, the super state <inline-formula id="S3.SS2.p2.m1"/> corresponds to a word in spoken language, which is the fundamental idea of the extension.
The <inline-formula id="S3.SS2.p2.m2"/>-th super state <inline-formula id="S3.SS2.p2.m3"/> has a phoneme sequence <inline-formula id="S3.SS2.p2.m4"/>, where <inline-formula id="S3.SS2.p2.m5"/> is the length of the <inline-formula id="S3.SS2.p2.m6"/>-th word <inline-formula id="S3.SS2.p2.m7"/>.
The generative process of the HDP-HLM is described as follows.</p>
        <p>
          <disp-formula-group id="S7.EGx2">
            <disp-formula id="S3.E10"/>
            <disp-formula id="S3.E11"/>
          </disp-formula-group>
        </p>
        <p>
          <disp-formula-group id="S7.EGx3">
            <disp-formula id="S3.E12"/>
            <disp-formula id="S3.E13"/>
          </disp-formula-group>
        </p>
        <p>
          <disp-formula-group id="S7.EGx4">
            <disp-formula id="S3.Ex1"/>
            <disp-formula id="S3.E14"/>
            <disp-formula id="S3.E15"/>
          </disp-formula-group>
        </p>
        <p>
          <disp-formula-group id="S7.EGx5">
            <disp-formula id="S3.E16"/>
            <disp-formula id="S3.E17"/>
            <disp-formula id="S3.E18"/>
            <disp-formula id="S3.E19"/>
            <disp-formula id="S3.E20"/>
          </disp-formula-group>
        </p>
        <p>
          <disp-formula-group id="S7.EGx6">
            <disp-formula id="S3.E21"/>
            <disp-formula id="S3.E22"/>
            <disp-formula id="S3.E23"/>
            <disp-formula id="S3.E24"/>
          </disp-formula-group>
        </p>
        <p>where <inline-formula id="S3.SS2.p2.m8"/> is the base measure and <inline-formula id="S3.SS2.p2.m9"/> and <inline-formula id="S3.SS2.p2.m10"/> are hyperparameters of a word model, which generates words, i.e., latent letter sequences.
Furthermore, <inline-formula id="S3.SS2.p2.m11"/> outputs <inline-formula id="S3.SS2.p2.m12"/>, representing the transition probability from latent letter <inline-formula id="S3.SS2.p2.m13"/> to the next latent letter. By contrast, <inline-formula id="S3.SS2.p2.m14"/> is the base measure, <inline-formula id="S3.SS2.p2.m15"/> and <inline-formula id="S3.SS2.p2.m16"/> are hyperparameters of the language model, and <inline-formula id="S3.SS2.p2.m17"/> outputs <inline-formula id="S3.SS2.p2.m18"/>, representing the transition probability from latent word <inline-formula id="S3.SS2.p2.m19"/> to the next latent word. The superscripts <inline-formula id="S3.SS2.p2.m20"/> and <inline-formula id="S3.SS2.p2.m21"/> indicate language model (LM) or word model (WM), respectively.
The latent letters contained in the <inline-formula id="S3.SS2.p2.m22"/>-th latent word <inline-formula id="S3.SS2.p2.m23"/> are sequentially sampled from <inline-formula id="S3.SS2.p2.m24"/>. The <inline-formula id="S3.SS2.p2.m25"/>-th latent letter of the <inline-formula id="S3.SS2.p2.m26"/>-th latent word is represented by <inline-formula id="S3.SS2.p2.m27"/>.</p>
        <p id="S3.SS2.p3">In contrast with HMMs, the duration distribution is explicitly determined for each latent letter <inline-formula id="S3.SS2.p3.m1"/> in the HDP-HLM. The HDP-HLM inherits this property from the HDP-HSMM <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref>. The duration time <inline-formula id="S3.SS2.p3.m2"/> of latent letter <inline-formula id="S3.SS2.p3.m3"/>, which is the <inline-formula id="S3.SS2.p3.m4"/>-th latent letter of the <inline-formula id="S3.SS2.p3.m5"/>-th latent word <inline-formula id="S3.SS2.p3.m6"/> in a sampled word sequence, is drawn from the duration distribution <inline-formula id="S3.SS2.p3.m7"/>, where <inline-formula id="S3.SS2.p3.m8"/> is the duration parameter for latent letter <inline-formula id="S3.SS2.p3.m9"/>.
The duration of a latent word <inline-formula id="S3.SS2.p3.m10"/> becomes <inline-formula id="S3.SS2.p3.m11"/>. When we assume that <inline-formula id="S3.SS2.p3.m12"/> is a Poisson distribution, <inline-formula id="S3.SS2.p3.m13"/> also follows a Poisson distribution whose parameter is <inline-formula id="S3.SS2.p3.m14"/> because of the properties of Poisson distributions.</p>
        <p id="S3.SS2.p4">In the HDP-HLM, latent word <inline-formula id="S3.SS2.p4.m1"/> determines a latent letter sequence <inline-formula id="S3.SS2.p4.m2"/>. Based on the determined sequence <inline-formula id="S3.SS2.p4.m3"/>, duration <inline-formula id="S3.SS2.p4.m4"/> of <inline-formula id="S3.SS2.p4.m5"/> is drawn, and observations <inline-formula id="S3.SS2.p4.m6"/> are drawn from an emission distribution <inline-formula id="S3.SS2.p4.m7"/> corresponding to <inline-formula id="S3.SS2.p4.m8"/>. The maps <inline-formula id="S3.SS2.p4.m9"/> and <inline-formula id="S3.SS2.p4.m10"/> represent the indices of words and letters, respectively, in a latent word sequence at time <inline-formula id="S3.SS2.p4.m11"/>. Using this generative model, a continuous time series data with a latent double articulation structure can be generated.
In this paper, we assume that observed time series data <inline-formula id="S3.SS2.p4.m12"/> represents a feature vector of the speech signal at time <inline-formula id="S3.SS2.p4.m13"/> and is generated in this way. Generally, the HDP-HLM can be applied to any kind of time series data that has a double articulation structure.</p>
        <p id="S3.SS2.p5">From the viewpoint of language acquisition, we review the generative model. In the conventional DAA <xref ref-type="bibr" rid="Taniguchi2011">[Taniguchi2011]</xref>, a DAA is composed of two separated machine learning methods, i.e., sticky HDP-HMM for encoding observation data to letter sequences and NPYLM for chunking letter sequences into word sequences.
On the one hand, the transition probabilities <inline-formula id="S3.SS2.p5.m1"/> and <inline-formula id="S3.SS2.p5.m2"/> correspond to the word bigram and letter bigram models in the NPYLM, respectively. Therefore, <inline-formula id="S3.SS2.p5.m3"/> contains information regarding a language model. On the other hand, <inline-formula id="S3.SS2.p5.m4"/> contains information regarding an acoustic model, which corresponds to a sticky HDP-HMM in conventional DAA.</p>
        <p id="S3.SS2.p6">The HDP-HLM assumes that the language model consists of a word bigram model. Mochihashi et al. compared the bigram and trigram language models and showed that the trigram assumption hardly improved the word segmentation performance although computational cost and complexity increased  <xref ref-type="bibr" rid="Mochihashi2009">[Mochihashi2009]</xref>. Therefore, the bigram assumption must be appropriate for a word segmentation and word discovery task.
</p>
        <p id="S3.SS2.p7">If we derive an efficient inference procedure for this two-layer hierarchical generative model, the inference procedure can infer the acoustic model and language model simultaneously.</p>
      </sec>
    </sec>
    <sec id="S4">
      <title>Inference algorithm</title>
      <p id="S4.p1">In this section, we derive an approximated blocked Gibbs sampler for the HDP-HLM. The sampler can simultaneously infer latent letters, latent words, a language model, and an acoustic model. Concurrently, the inference procedure can estimate the overall double articulation structure from continuous time series data. Therefore, we propose the unsupervised machine learning method NPB-DAA.
The overall inference procedure is shown in Algorithm 1.
</p>
      <sec id="S4.SS1">
        <title>Inference of latent words: <inline-formula id="S4.SS1.m1"/></title>
        <p id="S4.SS1.p1">In the HDP-HSMM, a backward filtering forward sampling procedure is adopted instead of the direct assignment procedure. When each latent state strongly depends on other neighboring latent states, the direct assignment procedure, which is a naive implementation of the Gibbs sampler, results in a poor mixing rate <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref>.
Johnson et al. showed that a blocked Gibbs sampler using a backward filtering forward sampling procedure that can simultaneously sample all hidden states of an observed sequence outperforms a direct-assignment Gibbs sampler.
By extending the backward filtering forward-sampling procedure and making it applicable to HDP-HLM, we can obtain an inference procedure for HDP-HLM.</p>
        <p id="S4.SS1.p2">The calculation of the backward messages for super states <inline-formula id="S4.SS1.p2.m1"/> in HDP-HSMM is as follows.</p>
        <p>
          <disp-formula-group id="S7.EGx7">
            <disp-formula id="S4.E25"/>
            <disp-formula id="S4.E26"/>
            <disp-formula id="S4.E27"/>
            <disp-formula id="S4.Ex2"/>
            <disp-formula id="S4.E28"/>
            <disp-formula id="S4.E29"/>
          </disp-formula-group>
        </p>
        <p>where <inline-formula id="S4.SS1.p2.m2"/> is a variable indicating that <inline-formula id="S4.SS1.p2.m3"/> is the boundary of the super state. If <inline-formula id="S4.SS1.p2.m4"/>, <inline-formula id="S4.SS1.p2.m5"/>. The variable <inline-formula id="S4.SS1.p2.m6"/> in (<xref rid="LABEL:eq:1">25</xref>) represents the probability that the latent super state <inline-formula id="S4.SS1.p2.m7"/> and that it transitions into a different super state at the next time step. Probability <inline-formula id="S4.SS1.p2.m8"/> is obtained by marginalizing over all super states <inline-formula id="S4.SS1.p2.m9"/> at time step <inline-formula id="S4.SS1.p2.m10"/>. Variable <inline-formula id="S4.SS1.p2.m11"/> in (<xref rid="LABEL:eq:2">27</xref>) represents the probability that the latent super state becomes <inline-formula id="S4.SS1.p2.m12"/> from time step <inline-formula id="S4.SS1.p2.m13"/>. This probability can be obtained by marginalizing over the duration variable in (<xref rid="LABEL:eq:4">28</xref>). Probability <inline-formula id="S4.SS1.p2.m14"/> in (<xref rid="LABEL:eq:4">28</xref>) shows the emission probability of observed data <inline-formula id="S4.SS1.p2.m15"/> given the condition that the duration <inline-formula id="S4.SS1.p2.m16"/> of <inline-formula id="S4.SS1.p2.m17"/> is <inline-formula id="S4.SS1.p2.m18"/>.
In the HDP-HSMM, all time steps with the same super state <inline-formula id="S4.SS1.p2.m19"/> share the same emission distribution. Therefore, the likelihood of a super state <inline-formula id="S4.SS1.p2.m20"/>, i.e., <inline-formula id="S4.SS1.p2.m21"/>, can be calculated easily.
</p>
        <p id="S4.SS1.p3">Surprisingly, in HDP-HLM, the exact same procedure of calculating backward messages as that of HDP-HSMM can be used.
We obtain a message passing algorithm for HDP-HLM by replacing a super state <inline-formula id="S4.SS1.p3.m1"/> in HDP-HSMM with latent word <inline-formula id="S4.SS1.p3.m2"/> in HDP-HLM. Only the likelihood of the latent word <inline-formula id="S4.SS1.p3.m3"/>, i.e., <inline-formula id="S4.SS1.p3.m4"/>, is different between the two message passing algorithms.
The likelihood of the occurrence of latent word <inline-formula id="S4.SS1.p3.m5"/> then becomes</p>
        <p>
          <disp-formula-group id="S7.EGx8">
            <disp-formula id="S4.Ex3"/>
            <disp-formula id="S4.E30"/>
            <disp-formula id="S4.E31"/>
          </disp-formula-group>
        </p>
        <p>where <inline-formula id="S4.SS1.p3.m6"/> indicates the number of elements in vector <inline-formula id="S4.SS1.p3.m7"/>, and <inline-formula id="S4.SS1.p3.m8"/> is an <inline-formula id="S4.SS1.p3.m9"/>-partition of duration <inline-formula id="S4.SS1.p3.m10"/>.
By substituting (<xref rid="LABEL:eq:5">30</xref>) into (<xref rid="LABEL:eq:4">28</xref>), we can obtain a formula to calculate the backward message of HDP-HLM.</p>
        <p id="S4.SS1.p4">The calculation of (<xref rid="LABEL:eq:5">30</xref>) looks complicated at first glance. However, the
value of (<xref rid="LABEL:eq:5">30</xref>) can be efficiently calculated using dynamic programming. If we define forward message <inline-formula id="S4.SS1.p4.m1"/> as the probability that the <inline-formula id="S4.SS1.p4.m2"/>-th latent letter in the relevant latent word <inline-formula id="S4.SS1.p4.m3"/> transits to the next latent letter at time <inline-formula id="S4.SS1.p4.m4"/> after emitting observations, forward message <inline-formula id="S4.SS1.p4.m5"/> can be recursively calculated as follows:</p>
        <p>
          <disp-formula-group id="S7.EGx9">
            <disp-formula id="S4.E32"/>
            <disp-formula id="S4.E33"/>
          </disp-formula-group>
        </p>
        <p>By applying the calculation formula shown above, backward messages <inline-formula id="S4.SS1.p4.m6"/> and <inline-formula id="S4.SS1.p4.m7"/> can be calculated. Using the calculation procedure for backward messages, the forward sampling procedure proposed in the HDP-HSMM can be employed. The backward filtering forward sampling procedure enables the blocked Gibbs sampler to directly sample latent words from observation data without explicitly sampling latent letters in HDP-HLM.</p>
        <p id="S4.SS1.p5">In the forward sampling procedure, super state <inline-formula id="S4.SS1.p5.m1"/> and its duration <inline-formula id="S4.SS1.p5.m2"/> are sampled iteratively using backward messages as follows.</p>
        <p>
          <disp-formula-group id="S7.EGx10">
            <disp-formula id="S4.Ex4"/>
            <disp-formula id="S4.E34"/>
            <disp-formula id="S4.Ex5"/>
            <disp-formula id="S4.E35"/>
          </disp-formula-group>
        </p>
        <p>where <inline-formula id="S4.SS1.p5.m3"/>.
For further details, please refer to the original paper, in which the HDP-HSMM were introduced <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref>.</p>
      </sec>
      <sec id="S4.SS2">
        <title>Sampling a letter sequence for a latent word: <inline-formula id="S4.SS2.m1"/></title>
        <p id="S4.SS2.p1">The sampled <inline-formula id="S4.SS2.p1.m1"/> is only an index of a latent word. Concrete letter sequences <inline-formula id="S4.SS2.p1.m2"/> for each latent word <inline-formula id="S4.SS2.p1.m3"/> should be sampled according to the correspondence of each sub-sequence of time series data <inline-formula id="S4.SS2.p1.m4"/> to each latent word.
When a latent word <inline-formula id="S4.SS2.p1.m5"/> is given, the generative model of the observation in the range of a latent word <inline-formula id="S4.SS2.p1.m6"/> can be regarded as an HDP-HSMM whose super states correspond to latent letters. Therefore, in the proposed model, each sub-sequence of observation data corresponding to a latent word can be considered an observed sequence generated by an HDP-HSMM.
If only a single sub-sequence of observations corresponds to a latent word, a latent letter sequence could be sampled using an ordinal sampling procedure in the HDP-HSMM. However, observations containing the same latent word have to share the same latent letter sequence <inline-formula id="S4.SS2.p1.m7"/>. Therefore, latent letter sequences for observations with the same latent word are simultaneously sampled, given that they have the same latent letter sequence.
We employ an approximate sampling procedure based on sampling importance resampling (SIR) <xref ref-type="bibr" rid="p_robotics">[p_robotics]</xref>.</p>
        <p id="S4.SS2.p2">If we define the observations sharing the same latent word as <inline-formula id="S4.SS2.p2.m1"/> and the shared latent letter sequence as <inline-formula id="S4.SS2.p2.m2"/>, the posterior probability <inline-formula id="S4.SS2.p2.m3"/> becomes</p>
        <p>
          <disp-formula-group id="S7.EGx11">
            <disp-formula id="S4.E36"/>
            <disp-formula id="S4.E37"/>
          </disp-formula-group>
        </p>
        <p>where <inline-formula id="S4.SS2.p2.m4"/> in (<xref rid="LABEL:eq:8">37</xref>), representing the likelihood of the observation, can be calculated using the backward filtering procedure in the HDP-HSMM. Probability <inline-formula id="S4.SS2.p2.m5"/> can also be calculated in the same way as (<xref rid="LABEL:eq:5">30</xref>) if <inline-formula id="S4.SS2.p2.m6"/> is given. The HDP-HSMM also provides a sampling procedure for <inline-formula id="S4.SS2.p2.m7"/>. Therefore, if we consider <inline-formula id="S4.SS2.p2.m8"/> as the proposed distribution and <inline-formula id="S4.SS2.p2.m9"/> as a weight, the SIR procedure can be employed <xref ref-type="bibr" rid="p_robotics">[p_robotics]</xref>.
Specifically, after a set of <inline-formula id="S4.SS2.p2.m10"/> are sampled from the proposed distribution <inline-formula id="S4.SS2.p2.m11"/>, a final sample is drawn from the set with a probability proportional to each sample’s weight.
Using this procedure, the proposed model can approximately sample a latent letter sequence <inline-formula id="S4.SS2.p2.m12"/> for the <inline-formula id="S4.SS2.p2.m13"/>-th latent word.</p>
      </sec>
      <sec id="S4.SS3">
        <title>Sampling model parameters</title>
        <p id="S4.SS3.p1">After sampling latent words <inline-formula id="S4.SS3.p1.m1"/> for each observation data and sampling letter sequences for the latent words, other parameters can be updated.
Parameters of the language model, i.e., <inline-formula id="S4.SS3.p1.m2"/> and <inline-formula id="S4.SS3.p1.m3"/>, can be updated on the basis of latent word sequences.
Parameters of the word model, i.e., <inline-formula id="S4.SS3.p1.m4"/> and <inline-formula id="S4.SS3.p1.m5"/>, can be updated on the basis of sampled letter sequences for latent words.
Parameters for the acoustic model, i.e., <inline-formula id="S4.SS3.p1.m6"/> and <inline-formula id="S4.SS3.p1.m7"/>, can be updated if each hidden state <inline-formula id="S4.SS3.p1.m8"/> is determined for each <inline-formula id="S4.SS3.p1.m9"/>. During the SIR process for sampling a letter sequence, <inline-formula id="S4.SS3.p1.m10"/> in Algorithm 1 are subsidiarily obtained. To accelerate the mixing rate, the subsidiary sampling results <inline-formula id="S4.SS3.p1.m11"/> obtained in the SIR are used for updating the acoustic model parameters.
These parameters can be sampled in the same way as the HDP-HSMM. For more details, we refer to the original paper in which the HDP-HSMM were introduced <xref ref-type="bibr" rid="johnson2013">[johnson2013]</xref>.
Finally, the overall sampling procedure is obtained, as described in Algorithm <xref rid="LABEL:alg:bgibbs">4.3</xref>.</p>
        <boxed-text id="algorithm1">
          <caption>
            <p>Blocked Gibbs sampler for HDP-HLM</p>
          </caption>
          <p>
            <list list-type="bullet">
              <list-item id="I1.i1">
                <p id="I1.i1.p1">Initialize all parameters.</p>
              </list-item>
              <list-item id="I1.i2">
                <p id="I1.i2.p1">Observe <inline-formula id="I1.i2.p1.m1"/> time series data <inline-formula id="I1.i2.p1.m2"/>.</p>
              </list-item>
              <list-item id="I1.i3">
                <p id="I1.i3.p1">
                  <bold>repeat</bold>
                </p>
                <p>
                  <list list-type="bullet">
                    <list-item id="I1.I1.i1">
                      <p id="I1.I1.i1.p1"><bold>for</bold> <inline-formula id="I1.I1.i1.p1.m1"/> to <inline-formula id="I1.I1.i1.p1.m2"/> <bold>do</bold></p>
                      <p>
                        <list list-type="bullet">
                          <list-item id="I1.I1.I1.i1">
                            <p id="I1.I1.I1.i1.p1">// Backward filtering procedure</p>
                          </list-item>
                          <list-item id="I1.I1.I1.i2">
                            <p id="I1.I1.I1.i2.p1">For each <inline-formula id="I1.I1.I1.i2.p1.m1"/>, initialize messages <inline-formula id="I1.I1.I1.i2.p1.m2"/>.</p>
                          </list-item>
                          <list-item id="I1.I1.I1.i3">
                            <p id="I1.I1.I1.i3.p1"><bold>for</bold> <inline-formula id="I1.I1.I1.i3.p1.m1"/> to <inline-formula id="I1.I1.I1.i3.p1.m2"/> <bold>do</bold></p>
                            <p>
                              <list list-type="bullet">
                                <list-item id="I1.I1.I1.I1.i1">
                                  <p id="I1.I1.I1.I1.i1.p1">For each <inline-formula id="I1.I1.I1.I1.i1.p1.m1"/>, compute backward messages <inline-formula id="I1.I1.I1.I1.i1.p1.m2"/> and <inline-formula id="I1.I1.I1.I1.i1.p1.m3"/> using (<xref rid="LABEL:eq:1">25</xref>)–(<xref rid="LABEL:eq:4">28</xref>).
</p>
                                </list-item>
                              </list>
                            </p>
                          </list-item>
                          <list-item id="I1.I1.I1.i4">
                            <p id="I1.I1.I1.i4.p1"><bold>end</bold> <bold>for</bold></p>
                          </list-item>
                          <list-item id="I1.I1.I1.i5">
                            <p id="I1.I1.I1.i5.p1">// Forward sampling procedure</p>
                          </list-item>
                          <list-item id="I1.I1.I1.i6">
                            <p id="I1.I1.I1.i6.p1">Initialize <inline-formula id="I1.I1.I1.i6.p1.m1"/> and <inline-formula id="I1.I1.I1.i6.p1.m2"/></p>
                          </list-item>
                          <list-item id="I1.I1.I1.i7">
                            <p id="I1.I1.I1.i7.p1"><bold>while</bold> <inline-formula id="I1.I1.I1.i7.p1.m1"/> <bold>do</bold></p>
                            <p>
                              <list list-type="bullet">
                                <list-item id="I1.I1.I1.I2.i1">
                                  <p id="I1.I1.I1.I2.i1.p1">// Sampling a super state representing a latent word</p>
                                </list-item>
                                <list-item id="I1.I1.I1.I2.i2">
                                  <p id="I1.I1.I1.I2.i2.p1">
                                    <inline-formula id="I1.I1.I1.I2.i2.p1.m1"/>
                                  </p>
                                </list-item>
                                <list-item id="I1.I1.I1.I2.i3">
                                  <p id="I1.I1.I1.I2.i3.p1">// Sampling duration of the super state</p>
                                </list-item>
                                <list-item id="I1.I1.I1.I2.i4">
                                  <p id="I1.I1.I1.I2.i4.p1">
                                    <inline-formula id="I1.I1.I1.I2.i4.p1.m1"/>
                                  </p>
                                </list-item>
                                <list-item id="I1.I1.I1.I2.i5">
                                  <p id="I1.I1.I1.I2.i5.p1">
                                    <inline-formula id="I1.I1.I1.I2.i5.p1.m1"/>
                                  </p>
                                </list-item>
                                <list-item id="I1.I1.I1.I2.i6">
                                  <p id="I1.I1.I1.I2.i6.p1">
                                    <inline-formula id="I1.I1.I1.I2.i6.p1.m1"/>
                                  </p>
                                </list-item>
                              </list>
                            </p>
                          </list-item>
                          <list-item id="I1.I1.I1.i8">
                            <p id="I1.I1.I1.i8.p1"><bold>end</bold> <bold>while</bold></p>
                          </list-item>
                          <list-item id="I1.I1.I1.i9">
                            <p id="I1.I1.I1.i9.p1">
                              <inline-formula id="I1.I1.I1.i9.p1.m1"/>
                            </p>
                          </list-item>
                          <list-item id="I1.I1.I1.i10">
                            <p id="I1.I1.I1.i10.p1">// Sampling a tentative latent letter sequences</p>
                          </list-item>
                          <list-item id="I1.I1.I1.i11">
                            <p id="I1.I1.I1.i11.p1"><bold>for</bold> <inline-formula id="I1.I1.I1.i11.p1.m1"/> to <inline-formula id="I1.I1.I1.i11.p1.m2"/> <bold>do</bold></p>
                            <p>
                              <list list-type="bullet">
                                <list-item id="I1.I1.I1.I3.i1">
                                  <p id="I1.I1.I1.I3.i1.p1">
                                    <inline-formula id="I1.I1.I1.I3.i1.p1.m1"/>
                                  </p>
                                </list-item>
                              </list>
                            </p>
                          </list-item>
                          <list-item id="I1.I1.I1.i12">
                            <p id="I1.I1.I1.i12.p1"><bold>end</bold> <bold>for</bold></p>
                          </list-item>
                        </list>
                      </p>
                    </list-item>
                    <list-item id="I1.I1.i2">
                      <p id="I1.I1.i2.p1"><bold>end</bold> <bold>for</bold></p>
                    </list-item>
                    <list-item id="I1.I1.i3">
                      <p id="I1.I1.i3.p1">// Update model parameters</p>
                    </list-item>
                    <list-item id="I1.I1.i4">
                      <p id="I1.I1.i4.p1">Sample acoustic model parameters <inline-formula id="I1.I1.i4.p1.m1"/> on the basis of tentatively sampled latent letter sequences <inline-formula id="I1.I1.i4.p1.m2"/>.</p>
                    </list-item>
                    <list-item id="I1.I1.i5">
                      <p id="I1.I1.i5.p1">Sample language model parameter <inline-formula id="I1.I1.i5.p1.m1"/> on the basis of sampled super states , i.e., latent words.</p>
                    </list-item>
                    <list-item id="I1.I1.i6">
                      <p id="I1.I1.i6.p1">Sample a word inventory <inline-formula id="I1.I1.i6.p1.m1"/> using SIR procedure (see (<xref rid="LABEL:eq:8">37</xref>)).</p>
                    </list-item>
                    <list-item id="I1.I1.i7">
                      <p id="I1.I1.i7.p1">Sample a word model <inline-formula id="I1.I1.i7.p1.m1"/> on the basis of sampled word inventory <inline-formula id="I1.I1.i7.p1.m2"/>.</p>
                    </list-item>
                  </list>
                </p>
              </list-item>
              <list-item id="I1.i4">
                <p id="I1.i4.p1"><bold>until</bold> a predetermined exit condition is satisfied.</p>
              </list-item>
            </list>
          </p>
        </boxed-text>
      </sec>
      <sec id="S4.SS4">
        <title>NPB-DAA</title>
        <p id="S4.SS4.p1">Based on the generative model, HDP-HLM, and its inference algorithm shown in Algorithm <xref rid="LABEL:alg:bgibbs">4.3</xref>, the proposed NPB-DAA is obtained, finally.
By assuming HDP-HLM as a generative model of observed time series data, and by inferring latent variables of the model, we can analyze latent double articulation structure, i.e., hierarchically organized latent words and phonemes, of the data in an unsupervised manner. We call the novel unsupervised double articulation analyzer NPB-DAA.</p>
      </sec>
    </sec>
    <sec id="S5">
      <title>Experiment 1: Synthetic data</title>
      <p id="S5.p1">We conducted an experiment using a synthetic dataset that explicitly has a double articulation structure to validate our proposed method.</p>
      <sec id="S5.SS1">
        <title>Conditions</title>
        <p id="S5.SS1.p1">To validate the ability of our proposed method to infer a latent double articulation structure in time series data, we applied the proposed NPB-DAA based on the HDP-HLM to synthetic time series data.
The conventional DAA was employed as a comparative method.
The time series data are generated using five letters <inline-formula id="S5.SS1.p1.m1"/> and four words <inline-formula id="S5.SS1.p1.m2"/> where <inline-formula id="S5.SS1.p1.m3"/> is a set of letters and <inline-formula id="S5.SS1.p1.m4"/> is a set of words. The four words were generated randomly.
The sequence <inline-formula id="S5.SS1.p1.m5"/> represents a word that is generated by combining <inline-formula id="S5.SS1.p1.m6"/> sequentially where <inline-formula id="S5.SS1.p1.m7"/> denotes the <inline-formula id="S5.SS1.p1.m8"/>-th letter of <inline-formula id="S5.SS1.p1.m9"/>. The durations of the letters were assumed to follow Poisson distributions and their parameters were drawn from a Gamma distribution whose parameters were <inline-formula id="S5.SS1.p1.m10"/>. The emission distribution was assumed to be a Gaussian distribution whose parameters were <inline-formula id="S5.SS1.p1.m11"/>, where <inline-formula id="S5.SS1.p1.m12"/> represents the index of latent letters. The variance of the emission distribution was changed in stages, and the inference results were compared. Forty time series data items were generated from 20 types of latent word sequences. Sixteen of them were pairs of words in <inline-formula id="S5.SS1.p1.m13"/>, e.g., <inline-formula id="S5.SS1.p1.m14"/> , and <inline-formula id="S5.SS1.p1.m15"/>. Four of them were three-word sentences, e.g., <inline-formula id="S5.SS1.p1.m16"/>. A sequence of latent words is represented by <inline-formula id="S5.SS1.p1.m17"/>. Two observations were generated from each word sequence.</p>
        <p id="S5.SS1.p2">We set the parameters of the NPB-DAA as follows: the hyperparameters for the latent language model were <inline-formula id="S5.SS1.p2.m1"/>, and the maximum number of words was six for weak-limit approximation. The hyperparameters for the latent word model were <inline-formula id="S5.SS1.p2.m2"/>, and the maximum number of letters was seven for weak-limit approximation. The hyperparameters of the duration distributions were set to <inline-formula id="S5.SS1.p2.m3"/> and <inline-formula id="S5.SS1.p2.m4"/>, and those of the emission distributions were set to <inline-formula id="S5.SS1.p2.m5"/>. The Gibbs Sampling procedure was iterated 100 times.</p>
        <p id="S5.SS1.p3">For the comparative model and conventional DAA, we set the hyperparameters of the sticky HDP-HMM to be as similar to those of the NPB-DAA as possible.</p>
      </sec>
      <sec id="S5.SS2">
        <title>Results</title>
        <p id="S5.SS2.p1">The average log-likelihood is shown in Fig. <xref rid="LABEL:fig:llk">4</xref>, where error bars represent the standard deviation of 30 trials. These results show that the proposed inference procedure worked appropriately, gradually sampling more probable latent variables as the iterations increased.
</p>
        <p id="S5.SS2.p2">In contrast with ordinal speech recognition tasks, the target task (language acquisition and double articulation analysis) is an unsupervised learning task. Specifically, it is a clustering task. Therefore, it is difficult to evaluate the methods’ performance from the viewpoint of precision and recall because the estimated index of a cluster and the label corresponding to the ground truth data are usually different. We evaluated the obtained result using the adjusted rand index (ARI), which quantifies the performance of a clustering task  <xref ref-type="bibr" rid="ari">[ari]</xref>.</p>
        <p id="S5.SS2.p3">Table <xref rid="LABEL:exp1_ari_letter">2</xref> shows the ARI for estimated latent letters, and
Table <xref rid="LABEL:exp1_ari_word">2</xref> shows the ARI for estimated latent words. Although
the ARI for the latent letters obtained by conventional DAA decreases when the variance <inline-formula id="S5.SS2.p3.m1"/> increases, that of NPB-DAA did not decrease as much. As the ARIs for latent words show, the performance of word segmentation by conventional DAA was poor, even when the ARI for latent letters was larger than <inline-formula id="S5.SS2.p3.m2"/>. In contrast, the ARI for latent words estimated by NPB-DAA was over <inline-formula id="S5.SS2.p3.m3"/> in all stages.
This shows that the NPB-DAA can mitigate the ill effects of phoneme recognition errors in the word segmentation task, and obtained knowledge about words can improve phoneme recognition performance.
Fig. <xref rid="LABEL:fig:aritime">5</xref> shows the change in ARI through iterations in the case of <inline-formula id="S5.SS2.p3.m4"/>. This shows that the ARI also increased gradually while log likelihood increases, as in Fig. <xref rid="LABEL:fig:llk">4</xref>.</p>
        <p id="S5.SS2.p4">An example of estimated latent variables is shown in Fig.  <xref rid="LABEL:fig:state1">6</xref>, which shows the results for time series data generated from the latent word sequence <inline-formula id="S5.SS2.p4.m1"/>. The input time series data is shown at very top of the figure. The top of each panel shows the true latent letters or latent words, whereas the panel beneath shows the inferred results. The vertical axes represent the iteration of the Gibbs sampling. In Fig. <xref rid="LABEL:fig:state1">6</xref>, the figure in the middle shows a latent word sequence estimated using the proposed method, and the figure at the bottom shows the estimated boundaries of the latent words. These results show that the inference procedure works consistently and can estimate an adequate boundary for the latent words given the data.</p>
        <fig id="S5.F4">
          <caption>
            <p>Log-likelihood profile through Gibbs sampling (<inline-formula id="S5.F4.m2"/>)</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="llk_s1.0_.eps" id="S5.F4.g1"/>
        </fig>
        <fig id="S5.F5">
          <caption>
            <p>ARI profile through Gibbs sampling (<inline-formula id="S5.F5.m2"/>)</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ARITIME_s1.0.eps" id="S5.F5.g1"/>
        </fig>
        <fig id="S5.F6">
          <caption>
            <p>Example of inference results for sample data <inline-formula id="S5.F6.m3"/> and <inline-formula id="S5.F6.m4"/>: (top) observation data, (upper middle) latent letters, (lower middle) latent words, and (bottom) the boundaries of latent words. Different colors denote different states.</p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="s1_15.eps" id="S5.F6.g1"/>
        </fig>
        <p>
          <table-wrap id="S5.T2"><caption><p>ARI for estimated latent letters</p></caption><caption><p>ARI for estimated latent words</p></caption>
<table>
<thead>
<tr>
<th><inline-formula id="S5.T2.m1"/></th>
<th>0.1</th>
<th>0.5</th>
<th>1.0</th></tr>
</thead>
<tbody>
<tr>
<th><p>Conventional DAA</p><p>(sticky HDP-HMM)</p></th>
<td>0.845</td>
<td>0.832</td>
<td>0.649</td></tr>
<tr>
<th>NPB-DAA</th>
<td><underline>0.984</underline></td>
<td><underline>0.895</underline></td>
<td><underline>0.938</underline></td></tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th><inline-formula id="S5.T2.m2"/></th>
<th><inline-formula id="S5.T2.m3"/></th>
<th><inline-formula id="S5.T2.m4"/></th>
<th><inline-formula id="S5.T2.m5"/></th></tr>
</thead>
<tbody>
<tr>
<th><p>Conventional DAA</p><p>(sticky HDP-HMM + NPYLM)</p></th>
<td>0.122</td>
<td>0.107</td>
<td>0.125</td></tr>
<tr>
<th>NPB-DAA</th>
<td><underline>0.594</underline></td>
<td><underline>0.509</underline></td>
<td><underline>0.618</underline></td></tr>
</tbody>
</table></table-wrap>
        </p>
        <p id="S5.SS2.p5">These results show that the proposed method is a more effective machine learning method for estimating a latent double articulation structure embedded in time series data.</p>
      </sec>
    </sec>
    <sec id="S6">
      <title>Experiment 2: Continuous Japanese Vowel Speech Signal</title>
      <p id="S6.p1">In the second experiment, we evaluated our proposed method using Japanese vowel speech signals to test the applicability of the proposed method to actual human continuous speech signal.</p>
      <sec id="S6.SS1">
        <title>Conditions</title>
        <p id="S6.SS1.p1">We prepared 60 audio data items. We asked a female Japanese speaker to read 30 artificial sentences aloud two times at a natural speed and recorded her voice.
The 30 sentences were prepared using five words {aioi, aue, ao, ie, uo}, which consisted of five Japanese vowels {a, i, u, e, o}. By reordering the five words, we prepared 25 two-word sentences, e.g., “ao aioi,” “uo aue,” and “aioi aioi,” and five three-word sentences, i.e., “uo aue ie,” “ie ie uo,” “aue ao ie,” “ao ie ao,” and “aioi uo ie.” The set of two-word sentences consisted of all types of word pairs (<inline-formula id="S6.SS1.p1.m1"/>). The set of three-word sentences were generated randomly.</p>
        <p id="S6.SS1.p2">The recorded data were encoded into <inline-formula id="S6.SS1.p2.m1"/>-dimensional mel-frequency cepstrum coefficient (MFCC) time series data using the HMM Toolkit (HTK) <fn id="idp6298512"><p>Hidden Markov Model Toolkit: http://htk.eng.cam.ac.uk/</p></fn>. The frame size and shift were set to <inline-formula id="S6.SS1.p2.m2"/> and <inline-formula id="S6.SS1.p2.m3"/> ms, respectively. Twelve-dimensional MFCC data was obtained as input data by eliminating power information from the original 13-dimensional MFCC data. As a result, 12-dimensional time series data at a frame rate of <inline-formula id="S6.SS1.p2.m4"/> Hz were obtained.
</p>
        <p id="S6.SS1.p3">The hyperparameters for the latent language model were set to <inline-formula id="S6.SS1.p3.m1"/> and <inline-formula id="S6.SS1.p3.m2"/>, and the maximum number of words was set to seven for weak-limit approximation. The hyperparameters for the latent word model were <inline-formula id="S6.SS1.p3.m3"/> and <inline-formula id="S6.SS1.p3.m4"/>, and the maximum number of letters was seven for weak-limit approximation. The hyperparameters of the duration distributions were set to <inline-formula id="S6.SS1.p3.m5"/> and <inline-formula id="S6.SS1.p3.m6"/>, and those of the emission distributions were set to <inline-formula id="S6.SS1.p3.m7"/> and <inline-formula id="S6.SS1.p3.m8"/>dimension<inline-formula id="S6.SS1.p3.m9"/>.</p>
        <p id="S6.SS1.p4">For the conventional DAA, we set the hyperparameters of the sticky HDP-HMM to be as similar to those of the NPB-DAA as possible. The hyperparameters for the NPYLM used in the conventional DAA were set to <inline-formula id="S6.SS1.p4.m1"/> and <inline-formula id="S6.SS1.p4.m2"/>. The Gibbs sampling procedure was iterated 100 times. With different random number seeds, 20 trials were performed.</p>
        <p id="S6.SS1.p5">As a baseline method, we employed an open-source continuous speech recognition engine, Julius,<fn id="idp6401760"><p>Open-Source Large Vocabulary CSR Engine Julius: http://julius.sourceforge.jp/. The Linux binary dictation-kit-v4.3.1-linux.tgz was used in this experiment. The software encodes the recorded data into 36-dimensional MFCC data including dynamic features and uses them for speech recognition. </p></fn> which is widely used in Japanese speech recognition tasks. Julius’s acoustic model is trained by using a large number of speech data in a supervised manner. We prepared four conditions for Julius. The first one was called “Julius (phoneme + NPYLM).” In this condition, we used Julius as a phoneme recognition system by preparing a phoneme dictionary containing five Japanese vowels {a, i, u, e, o}.<fn id="idp6404400"><p>In addition, Julius’s dictionary also contains silB and silE to represent silence because of its system requirements.</p></fn> After encoding continuous speech signals into phoneme sequences using Julius as a phoneme recognizer, unsupervised morphological analysis based on the NPYLM was conducted to discover words and a language model. The second condition was called “Julius (phoneme + latticelm).” In this condition, we also used latticelm, which is an unsupervised morphological analyzer for lattice output from an ASR system. The method was proposed by Neubig et al. as an extension of Mochihashi’s NPYLM <xref ref-type="bibr" rid="Neubig2012">[Neubig2012]</xref>. In this condition, the latticelm software<fn id="idp6408512"><p>latticelm: http://www.phontron.com/latticelm/index.html</p></fn> developed by Neubig et al. was used.</p>
        <p id="S6.SS1.p6">In the third and fourth conditions, called “Julius (monophone + word dictionary)” and “Julius (triphone + word dictionary),” respectively. we prepared a complete word dictionary that contained all of the words that appeared in the target speech signal, i.e.,{aioi, aue, ao, ie, uo}, for Julius. This condition provides almost an upper bound for the performance of our task.
Except for in “Julius (triphone + word dictionary),” Julius uses a monophone-based acoustic model contained in the dictation kit. The acoustic model is trained in a supervised manner using a large number of labeled speech data. “Julius (triphone + word dictionary)” used a triphone-based acoustic model for comparison.</p>
      </sec>
      <sec id="S6.SS2">
        <title>Results</title>
        <p id="S6.SS2.p1">We provided word and letter ground truth labels to all frames of the speech signal data and evaluated the relationship between the truth labels and estimated latent letter and word indices.</p>
        <p>
          <table-wrap id="S6.T3"><caption><p>ARI for estimated latent letters and words</p></caption>
<table>
<tbody>
<tr>
<td>Method</td>
<td><p>Letter ARI</p></td>
<td><p>Word ARI</p></td>
<td>AM</td>
<td>LM</td></tr>
<tr>
<td>NPB-DAA (MAP)</td>
<td><bold>0.599</bold></td>
<td><bold>0.497</bold></td>
<td/>
<td/></tr>
<tr>
<td>NPB-DAA</td>
<td>0.574</td>
<td>0.385</td>
<td/>
<td/></tr>
<tr>
<td>Conventional DAA</td>
<td>0.584</td>
<td>0.072</td>
<td/>
<td/></tr>
<tr>
<td><p>Julius (phoneme dictionary</p><p>+ NPYLM)</p></td>
<td>0.483</td>
<td>0.315</td>
<td>✓</td>
<td/></tr>
<tr>
<td><p>Julius (phoneme dictionary</p><p>+ latticelm)</p></td>
<td>0.524</td>
<td>0.426</td>
<td>✓</td>
<td/></tr>
<tr>
<td><p>Julius (monophone</p><p>+ word dictionary)</p></td>
<td>0.565</td>
<td>0.548</td>
<td>✓</td>
<td>✓</td></tr>
<tr>
<td><p>Julius (triphone</p><p>+ word dictionary)</p></td>
<td>0.516</td>
<td>0.636</td>
<td>✓</td>
<td>✓</td></tr>
</tbody>
</table></table-wrap>
        </p>
        <p id="S6.SS2.p2">The results are shown in Table <xref rid="LABEL:tbl:exp2_ari">3</xref>. Check marks in the AM and LM columns indicate that the method used a pretrained acoustic model and given true language model, respectively.
We see that “NPB-DAA (MAP)” outperformed the conventional DAA. The results of “NPB-DAA” and “Conventional DAA” show the ARI averaged over 20 trials. In contrast, “NPB-DAA (MAP)” obtained the maximum a posteriori probability (MAP) of the 20 trials.
An advantage of the NPB-DAA is that the method can calculate the posterior probability of a given dataset after the learning phase because the NPB-DAA is derived from a generative model, i.e., HDP-HLM, which integrates the language and acoustic models. In contrast with the conventional DAA and similar methods that do not have appropriate generative models, the NPB-DAA can obtain an appropriate learning result by referring to the probability. The rows with MAP in Table <xref rid="LABEL:tbl:exp2_ari">3</xref> show that this probability is an adequate criterion for selecting a learning result.
</p>
        <p id="S6.SS2.p3">The results show that the NPB-DAA outperformed not only the conventional DAA but also Julius-based word discovery systems whose acoustic models were trained in supervised manner.
One reason is that the acoustic models of the DAAs were trained only from the female participant’s speech signals, in contrast, Julius’s acoustic model was trained by the speech signals of many speakers.
In other words, NPB-DAA acquired speaker-dependent acoustic model in contrast with that Julius used speaker-independent acoustic model.
This adaptation of acoustic model to the speaker must have increased the NPB-DAA’s performance.</p>
        <p id="S6.SS2.p4">The results show that a naive application of the NPYLM to recognized phoneme sequences results in poor word acquisition performance, especially in conventional DAA. Because the theory of the NPYLM does not presume that letter sequences have recognition errors, the existence of phoneme recognition error deteriorates word segmentation performance. The methods that simply apply an NPYLM to obtained phoneme sequences, i.e., the conventional DAA and Julius (phoneme dictionary + NPYLM), output bad results in the word ARI compared with those of the letter ARI. However, latticelm, which presumes phoneme recognition errors to some extent, could not dramatically improve the performance of word acquisition in our experimental setting.</p>
        <p id="S6.SS2.p5">In contrast, “Julius (triphone + word dictionary)” improved its word ARI performance with respect to letter ARI performance. “Julius (monophone + word dictionary)” also kept its performance high with respect to the word recognition task compared with the phoneme recognition task.
We note that the word error rate was 28.9 % and the phoneme error rate was 23.9 % in Julius (monophone + word dictionary).</p>
        <p id="S6.SS2.p6">In the research field of ASR, it is widely known that a good language model improves word and phoneme recognition performance. The NPB-DAA could not improve the performance of word ARI with respect to letter ARI performance. However, it obtained an adequate language model and prevented the score of the word ARI from becoming far worse than that of the letter ARI. To achieve such an error-proof word acquisition, the direct inference of latent words are important in NPB-DAA. In the inference procedure described in Section <xref rid="LABEL:sec3">3</xref>, latent words are sampled directly without sampling latent letters while marginalizing all possible latent letter sequences. This achieves an effect similar to that of a given language model in the inference process</p>
        <p id="S6.SS2.p7">Typical examples of the estimation results are shown in Table <xref rid="LABEL:tbl:DAAresult">4</xref> for NPB-DAA and conventional DAA. Each number in parentheses represents an estimated phoneme label, each space represents a phoneme boundary, each number in bold style represents a sampled index of a word, and “<inline-formula id="S6.SS2.p7.m1"/>” represents a boundary between successive words. For example, “ao ie” was divided into two words, i.e., “5 0 1” and “6 3 4 6,” in the NPB-DAA results, and their word indices were 3 and 4. In Table <xref rid="LABEL:tbl:DAAresult">4</xref>, the sampled letters corresponding to the word “ie” are underlined. Although conventional DAA could not estimate “ie” as a single word, the NPB-DAA could estimate “ie” to be a single word: “4.” In the conventional DAA results, several phoneme recognition errors can be found. The errors completely deteriorated the following chunking process, i.e., unsupervised morphological analysis using a NPYLM, as past research has frequently pointed out.
As shown in Table <xref rid="LABEL:tbl:DAAresult">4</xref>, NPB-DAA had some phoneme recognition errors. However, in the NPB-DAA, latent words are sampled on the basis of the marginalized phoneme distribution before sampling concrete phoneme sequences. This property of the sampling procedure seemed to improve the performance of NPB-DAA.</p>
        <p>
          <table-wrap id="S6.T4"><caption><p>Example word discovery results </p></caption>
<table>
<thead>
<tr>
<th>Vowel sequence</th>
<th>Estimated NPB-DAA results</th>
<th>Estimated conventional DAA results</th></tr>
</thead>
<tbody>
<tr>
<td>ao ie</td>
<td><bold>3</bold> (5 0 1) / <bold>4</bold> (6 3 4 6)</td>
<td><bold>226</bold> (2 0 3 4 1 5 4 1)</td></tr>
<tr>
<td>ao ie ao</td>
<td><bold>3</bold> (5 0 1) / <bold>4</bold> (6 3 4 6) / <bold>3</bold> (5 0 1) / <bold>0</bold> (6 4 6)</td>
<td><bold>494</bold> (3) / <bold>675</bold> (2 3 0) / <bold>374</bold> ( 1 5 4 1 2 0 1)</td></tr>
<tr>
<td>aue ie</td>
<td><bold>6</bold> (6 5 1 2 6 4) / <bold>4</bold> (6 3 4 6)</td>
<td><bold>329</bold> (2 3 8 4 5 4 1)</td></tr>
<tr>
<td>ie ie</td>
<td><bold>4</bold> (6 3 4 6) / <bold>4</bold> (6 3 4 6)</td>
<td><bold>389</bold> ( 5 4 1 41 5 4 1)</td></tr>
<tr>
<td>ie uo</td>
<td><bold>4</bold> (6 3 4 6) / <bold>5</bold> (5 1 2) / <bold>3</bold> (5 0 1) / <bold>0</bold> (0 6)</td>
<td><bold>401</bold> ( 5 4 1 8 0 1)</td></tr>
<tr>
<td>ie aioi</td>
<td><bold>4</bold> (6 3 4 6) / <bold>1</bold> (5 6 4 6 3 6 1) / <bold>4</bold> (6 3 4 6)</td>
<td><bold>813</bold> ( 5 4 1 2 4 5) / <bold>602</bold> (4 3 0 3 4 5)</td></tr>
</tbody>
</table></table-wrap>
        </p>
        <p id="S6.SS2.p8">An example of the estimated latent variables is shown in Fig.  <xref rid="LABEL:fig:npb_example_inference">7</xref>, which shows the results for time series data corresponding to a vowel sequence, “ao ie ao.” The input time series data, i.e., 12-dimensional MFCC time series data, are shown at the top of the figures. The middle and the bottom figures show the inference process. The top of each figure shows the true latent letters or latent words, whereas the bottom shows the inferred result. The vertical axes represent the number of Gibbs sampling iterations. This shows that the inference procedure worked for human vowel sequence data, and could estimate an adequate unit for each word.</p>
        <fig id="S6.F7">
          <caption>
            <p>Example of inference results for “ao ie ao.” MFCC feature vectors are plotted in the top panel. The middle and bottom panels show the inference results of latent letters and latent words, respectively. Different colors denotes different states. </p>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ao_ie_ao2.eps" id="S6.F7.g1"/>
        </fig>
      </sec>
    </sec>
    <sec id="S7">
      <title>Conclusion</title>
      <p id="S7.p1">In this paper, we proposed NPB-DAA for direct and simultaneous acquisition of language and acoustic models from continuous speech signals in an unsupervised manner. For this purpose, we proposed an integrative generative model called the HDP-HLM by extending HDP-HSMM. Based on the generative model, we derived an inference procedure by extending the blocked Gibbs sampler originally proposed for HDP-HSMM. The method is expected to enable a developmental robot to simultaneously obtain language and acoustic models directly from continuous speech signals. To evaluate the performance of the proposed method, two experiments were performed. In the first experiment, the proposed method was applied to synthetic data, and it was shown that the method can successfully infer latent words embedded in time series data in an unsupervised manner. In the second experiment, we applied the proposed method to actual human Japanese vowel sequences. The result showed that the proposed method outperformed a conventional two-stage sequential method, conventional DAA, and a baseline ASR method.</p>
      <p id="S7.p2">One of the most important challenges in our future work is to achieve complete human language acquisition from speech signals. We did not achieve complete language acquisition from speech signals that includes consonants as well as vowels in this study. Language acquisition from more natural speech signals like child-directed speech by human parents are also part of our future work. To achieve these aims, we still have two main problems: feature extraction and computational cost.</p>
      <p id="S7.p3">To address these problems, more sophisticated feature extraction methods are needed. Deep learning has gained attention recently because of its impressive feature extraction performance. Integrating a deep learning method into the NPB-DAA should improve its performance.</p>
      <p id="S7.p4">Computational cost is another problem. Even though the size of the
dataset used in the Experiment 2 was very small, it took approximately 240
minutes for 100 iterations using an Intel Xeon CPU E5-2650 v2 2.60 GHz, 8 cores <inline-formula id="S7.p4.m1"/> 16
CPU. In particular, the computational cost of the blocked Gibbs sampler was <inline-formula id="S7.p4.m2"/>, where <inline-formula id="S7.p4.m3"/> is the maximum number of latent letters for a word, <inline-formula id="S7.p4.m4"/> is the maximum duration of a word, and <inline-formula id="S7.p4.m5"/>is the maximum number of words.
To apply the proposed method to a larger dataset, improving its computational cost will be necessary.
</p>
      <p id="S7.p5">Currently, the accuracy of the language acquisition is still limited, as shown in Table <xref rid="LABEL:tbl:exp2_ari">3</xref>. In this paper, we focused on a language acquisition method based on distributional cues and proposed a mathematical model for language acquisition. Obviously, distributional cues are not enough for more accurate language acquisition. As suggested by several computational and robotic studies, making use of co-occurrence cues improves the accuracy of language acquisition <xref ref-type="bibr" rid="Nakamura2014">[Nakamura2014, Akira2015, Taguchi2011]</xref>. The proposed HDP-HLM is a fully probabilistic generative model. Therefore, introducing other factors into consideration is relatively easier than for other heuristic models. This is also advantage of our approach. Combining prosodic and co-occurrence cues into the NPB-DAA, and obtaining a more accurate and more plausible constructive developmental language acquisition model is also a direction for future research.
</p>
    </sec>
  </body>
  <back>
    <ack/>
    <glossary/>
    <ref-list>
      <title>References</title>
    </ref-list>
    <app-group/>
  </back>
</article>
