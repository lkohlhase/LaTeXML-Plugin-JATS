<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN" "archivearticle3.dtd">
<article>
  <front>
    <article-meta><title-group><article-title>A Geometric View on Constrained <inline-formula id="m1"><tex-math>M</tex-math></inline-formula>-Estimators</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Volkan Cevher</surname><given-names>Yen-Huan Li, 
			Ya-Ping Hsieh, 
			Nissim 
			Zerbib 
			and 
			</given-names></name></contrib></contrib-group><pub-date><string-date/></pub-date><abstract><p>We study the estimation error of constrained <inline-formula id="m2"><tex-math>M</tex-math></inline-formula>-estimators, and derive explicit upper bounds on the expected estimation error determined by the Gaussian width of the constraint set. Both of the cases where the true parameter is on the boundary of the constraint set (matched constraint), and where the true parameter is strictly in the constraint set (mismatched constraint) are considered. For both cases, we derive novel universal estimation error bounds for regression in a generalized linear model with the canonical link function. Our error bound for the mismatched constraint case is minimax optimal in terms of its dependence on the sample size, for Gaussian linear regression by the Lasso.</p></abstract>
	An error in the conversion from LaTeX to XML has occurred here. 
</article-meta>
  </front>
  <body>
	An error in the conversion from LaTeX to XML has occurred here. 
<p>LaboratoryforInformationandInferenceSystemsÉcolePolytechniqueFédéraledeLausanne</p><sec id="LABEL:sec_formulation"><title>Introduction</title><p>Consider a general statistical estimation problem. Let <inline-formula id="S1.p1.m1"><tex-math>(y_{1},\ldots,y_{n})</tex-math></inline-formula> be a sample following a probability distribution <inline-formula id="S1.p1.m2"><tex-math>\mathbb{P}_{\theta^{\natural}}</tex-math></inline-formula> in a given class <inline-formula id="S1.p1.m3"><tex-math>\mathcal{P}:=\left\{\mathbb{P}_{\theta}:\theta\in\mathbb{R}^{p}\right\}</tex-math></inline-formula>. We are interested in estimating the parameter <inline-formula id="S1.p1.m4"><tex-math>\theta^{\natural}</tex-math></inline-formula>, given <inline-formula id="S1.p1.m5"><tex-math>(y_{1},\ldots,y_{n})</tex-math></inline-formula> and <inline-formula id="S1.p1.m6"><tex-math>\mathcal{P}</tex-math></inline-formula>, under the high-dimensional setting where <inline-formula id="S1.p1.m7"><tex-math>n&lt;p</tex-math></inline-formula>.</p><p>If <inline-formula id="S1.p2.m1"><tex-math>\theta^{\natural}</tex-math></inline-formula> is known to satisfy <inline-formula id="S1.p2.m2"><tex-math>g(\theta^{\natural})\leq c</tex-math></inline-formula> for some continuous convex function <inline-formula id="S1.p2.m3"><tex-math>g</tex-math></inline-formula> and positive constant <inline-formula id="S1.p2.m4"><tex-math>c</tex-math></inline-formula>, we can consider a constrained <inline-formula id="S1.p2.m5"><tex-math>M</tex-math></inline-formula>-estimator of the form</p><p><disp-formula id="LABEL:eq_that"><tex-math>\hat{\theta}\in\arg\min_{\theta}\left\{f_{n}(\theta):\theta\in\mathcal{G}%
\right\},\quad\mathcal{G}:=\left\{\theta\in\mathbb{R}^{p}:g(\theta)\leq c%
\right\}.</tex-math></disp-formula></p><p>We assume that <inline-formula id="S1.p2.m6"><tex-math>f_{n}</tex-math></inline-formula> is a continuously differentiable convex function, and the constraint set <inline-formula id="S1.p2.m7"><tex-math>\mathcal{G}</tex-math></inline-formula> is non-empty. For example, the Lasso <xref ref-type="bibr" rid="Tibshirani1996">[Tibshirani1996]</xref> corresponds to</p><p><disp-formula id="LABEL:eq_LS"><tex-math>f_{n}(\theta):=\frac{1}{2n}\sum_{i=1}^{n}\left(y_{i}-\left\langle a_{i},\theta%
\right\rangle\right)^{2},\quad\mathcal{G}:=\left\{\left\|\theta\right\|_{1}%
\leq c\right\},</tex-math></disp-formula></p><p>for some <inline-formula id="S1.p2.m8"><tex-math>a_{1},\ldots,a_{n}\in\mathbb{R}^{p}</tex-math></inline-formula> and positive constant <inline-formula id="S1.p2.m9"><tex-math>c</tex-math></inline-formula>. A matrix <inline-formula id="S1.p2.m10"><tex-math>\Theta\in\mathbb{R}^{d\times d}</tex-math></inline-formula> can be vectorized as a corresponding vector <inline-formula id="S1.p2.m11"><tex-math>\theta\in\mathbb{R}^{p}</tex-math></inline-formula>, <inline-formula id="S1.p2.m12"><tex-math>d^{2}=p</tex-math></inline-formula>. In the low-rank matrix recovery problem <xref ref-type="bibr" rid="Candes2011b">[Candes2011b, Gunasekar2014]</xref>, a popular estimator corresponds to</p><p><disp-formula id="LABEL:eq_matrix_lasso"><tex-math>f_{n}(\Theta):=\frac{1}{2n}\sum_{i=1}^{n}\left(y_{i}-\mathrm{Tr}\left(A_{i}^{T%
}\Theta\right)\right)^{2},\quad\mathcal{G}:=\left\{\left\|\Theta\right\|_{*}%
\leq c\right\},</tex-math></disp-formula></p><p>for some <inline-formula id="S1.p2.m13"><tex-math>A_{1},\ldots,A_{n}\in\mathbb{R}^{d\times d}</tex-math></inline-formula> and positive constant <inline-formula id="S1.p2.m14"><tex-math>c</tex-math></inline-formula>, where <inline-formula id="S1.p2.m15"><tex-math>\left\|\cdot\right\|_{*}</tex-math></inline-formula> denotes the nuclear norm. In general, <inline-formula id="S1.p2.m16"><tex-math>f_{n}</tex-math></inline-formula> can be the normalized negative log-likelihood function, or any properly defined function, and <inline-formula id="S1.p2.m17"><tex-math>g</tex-math></inline-formula> depends on the <italic>a priori</italic> information on the structure of the parameter <inline-formula id="S1.p2.m18"><tex-math>\theta^{\natural}</tex-math></inline-formula><xref ref-type="bibr" rid="Bach2013a">[Bach2013a, Chandrasekaran2012, ElHalabi2015]</xref>.</p><p>One can also consider a penalized <inline-formula id="S1.p3.m1"><tex-math>M</tex-math></inline-formula>-estimator, given by</p><p><disp-formula id="LABEL:eq_penalized_est"><tex-math>\hat{\theta}_{\text{penalized}}\in\arg\min_{\theta\in\mathbb{R}^{p}}\left\{f_{%
n}(\theta)+\rho_{n}g(\theta)\right\},</tex-math></disp-formula></p><p>for some positive constant <inline-formula id="S1.p3.m2"><tex-math>\rho_{n}</tex-math></inline-formula>. The penalized <inline-formula id="S1.p3.m3"><tex-math>M</tex-math></inline-formula>-estimator can be computed by fast proximal methods, provided that the proximal mapping of <inline-formula id="S1.p3.m4"><tex-math>g</tex-math></inline-formula> is easy to compute <xref ref-type="bibr" rid="Beck2009">[Beck2009, Nesterov2013]</xref>. This condition, however, is not always satisfied. For example, if <inline-formula id="S1.p3.m5"><tex-math>g</tex-math></inline-formula> is the nuclear norm, computing the corresponding proximal mapping requires a full singular value decomposition (SVD) in the first few iterations, and hence is not scalable with the parameter dimension. In contrast, if we consider a constrained <inline-formula id="S1.p3.m6"><tex-math>M</tex-math></inline-formula>-estimator and compute it by the Frank-Wolfe algorithm, each iteration of the algorithm requires a linear minimization oracle (LMO), which can be approximated efficiently by Lanczos’ algorithm <xref ref-type="bibr" rid="Jaggi2013">[Jaggi2013]</xref>. The paper <xref ref-type="bibr" rid="Zhang2013">[Zhang2013]</xref> also shows that when <inline-formula id="S1.p3.m7"><tex-math>g</tex-math></inline-formula> is a structured sparsity regularizer, the LMO can be much easier to compute than the proximal mapping.
</p><p>If we consider a constrained <inline-formula id="S1.p4.m1"><tex-math>M</tex-math></inline-formula>-estimator, setting the value of the constant <inline-formula id="S1.p4.m2"><tex-math>c</tex-math></inline-formula> in (<xref ref-type="labelref" rid="LABEL:eq_that">1</xref>) becomes a practical issue.
For the case <inline-formula id="S1.p4.m3"><tex-math>c&lt;g(\theta^{\natural})</tex-math></inline-formula>, the estimation error is obviously bounded below by the distance between <inline-formula id="S1.p4.m4"><tex-math>\theta^{\natural}</tex-math></inline-formula> and the constraint set <inline-formula id="S1.p4.m5"><tex-math>\mathcal{G}</tex-math></inline-formula>, and hence estimation consistency is impossible.
Ideally we would like to set <inline-formula id="S1.p4.m6"><tex-math>c=g(\theta^{\natural})</tex-math></inline-formula>, while in practice <inline-formula id="S1.p4.m7"><tex-math>g(\theta^{\natural})</tex-math></inline-formula> is seldom known. The last case is when we have some estimate on <inline-formula id="S1.p4.m8"><tex-math>g(\theta^{\natural})</tex-math></inline-formula>, and choose <inline-formula id="S1.p4.m9"><tex-math>c</tex-math></inline-formula> such that <inline-formula id="S1.p4.m10"><tex-math>c&gt;g(\theta^{\natural})</tex-math></inline-formula>. Some natural questions arise: Is estimation consistency possible? How fast will the estimation error decay with the sample size <inline-formula id="S1.p4.m11"><tex-math>n</tex-math></inline-formula>? Does setting <inline-formula id="S1.p4.m12"><tex-math>c&gt;g(\theta^{\natural})</tex-math></inline-formula> result in larger estimation error than setting <inline-formula id="S1.p4.m13"><tex-math>c=g(\theta^{\natural})</tex-math></inline-formula>? We review related works in Section <xref ref-type="labelref" rid="LABEL:sec_related_work">2</xref>, which shows that answers existed only for specific cases even when <inline-formula id="S1.p4.m14"><tex-math>c=g(\theta^{\natural})</tex-math></inline-formula>.</p><p>In this paper, we provide a unified analysis for constrained <inline-formula id="S1.p5.m1"><tex-math>M</tex-math></inline-formula>-estimators.
Specifically,</p><list list-type="bullet" id="I1"><list-item id="I1.i1"><p>We propose an elementary framework for analyzing any <inline-formula id="I1.i1.p1.m1"><tex-math>M</tex-math></inline-formula>-estimator applied to any statistical model in Section <xref ref-type="labelref" rid="LABEL:sec_framework">3</xref>.</p></list-item><list-item id="I1.i2"><p>We obtain universal error bounds in terms of the Gaussian width, valid <italic>for all</italic> canonical GLMs. We consider the matched constraint case (<inline-formula id="I1.i2.p1.m1"><tex-math>c=g(\theta^{\natural})</tex-math></inline-formula>) in Section <xref ref-type="labelref" rid="LABEL:sec_matched">4</xref>, and the mismatched constraint case (<inline-formula id="I1.i2.p1.m2"><tex-math>c&gt;g(\theta^{\natural})</tex-math></inline-formula>) in Section <xref ref-type="labelref" rid="LABEL:sec_mismatch">5</xref>.</p></list-item><list-item id="I1.i3"><p>To illustrate the universal error bounds, we specialize the universal error bound to Gaussian linear regression with arbitrary convex constraint, and regression in canonical GLMs with the <inline-formula id="I1.i3.p1.m1"><tex-math>\ell_{1}</tex-math></inline-formula>-constraint in Section <xref ref-type="labelref" rid="LABEL:sec_appl">6</xref>, and obtain explicit results.</p></list-item><list-item id="I1.i4"><p>Our error bound for the Lasso applied to the Gaussian linear model is optimal in the minimax sense (cf. Section <xref ref-type="labelref" rid="LABEL:sec_mismatch_further">7</xref>).
</p></list-item></list><p>Existing results for penalized <inline-formula id="S1.p7.m1"><tex-math>M</tex-math></inline-formula>-estimators <xref ref-type="bibr" rid="Banerjee2015">[Banerjee2015, Bickel2009, Buhlmann2011, Honorio2014, Kakade2010, Negahban2012, Geer2013]</xref>, which are for deterministic <inline-formula id="S1.p7.m2"><tex-math>\rho_{n}</tex-math></inline-formula>’s, cannot directly recover our results, and vice versa.
Indeed, by Lagrange duality, there exists some <inline-formula id="S1.p7.m3"><tex-math>\rho_{n}&gt;0</tex-math></inline-formula> such that the constrained <inline-formula id="S1.p7.m4"><tex-math>M</tex-math></inline-formula>-estimator in (<xref ref-type="labelref" rid="LABEL:eq_that">1</xref>) is equivalent to the penalized <inline-formula id="S1.p7.m5"><tex-math>M</tex-math></inline-formula>-estimator in (<xref ref-type="labelref" rid="LABEL:eq_penalized_est">4</xref>).
This correspondence, however, holds <italic>only for given realization of the sample <inline-formula id="S1.p7.m6"><tex-math>(y_{1},\ldots,y_{n})</tex-math></inline-formula></italic>, and hence <inline-formula id="S1.p7.m7"><tex-math>\rho_{n}</tex-math></inline-formula> is a random variable depending on the sample.
Conversely, for any penalized <inline-formula id="S1.p7.m8"><tex-math>M</tex-math></inline-formula>-estimator <inline-formula id="S1.p7.m9"><tex-math>\hat{\theta}_{\text{penalized}}</tex-math></inline-formula> for some <inline-formula id="S1.p7.m10"><tex-math>\rho_{n}&gt;0</tex-math></inline-formula>, there exists a constant <inline-formula id="S1.p7.m11"><tex-math>c=g(\hat{\theta}_{\text{penalized}})</tex-math></inline-formula> such that the corresponding constrained <inline-formula id="S1.p7.m12"><tex-math>M</tex-math></inline-formula>-estimator (<xref ref-type="labelref" rid="LABEL:eq_penalized_est">4</xref>) is equivalent to <inline-formula id="S1.p7.m13"><tex-math>\hat{\theta}_{\text{penalized}}</tex-math></inline-formula>.
Note that <inline-formula id="S1.p7.m14"><tex-math>c=g(\hat{\theta}_{\text{penalized}})</tex-math></inline-formula> is again a random variable and dependent on the sample.
We are not aware of any existing work on characterizing the correspondence between the two formulations.</p></sec><sec id="LABEL:sec_related_work"><title>Related Works</title><p>In <xref ref-type="bibr" rid="Oymak2013a">[Oymak2013a, Oymak2013]</xref>, the authors derived sharp estimation error bounds for regression in the linear model by constrained least squares (LS) estimators.
The analysis in <xref ref-type="bibr" rid="Vershynin2014">[Vershynin2014]</xref> provides a minimax estimation error bound for the same setting
.
There are some related works on learning a function in a function class <xref ref-type="bibr" rid="Koltchinskii2013a">[Koltchinskii2013a, Mendelson2014]</xref>. When the function class is linearly parametrized by vectors in <inline-formula id="S2.p1.m1"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>, and the function corresponding to <inline-formula id="S2.p1.m2"><tex-math>\theta^{\natural}</tex-math></inline-formula> is in the function class, the <inline-formula id="S2.p1.m3"><tex-math>L_{2}</tex-math></inline-formula>-estimation error in the function class may be translated into the <inline-formula id="S2.p1.m4"><tex-math>\ell_{2}</tex-math></inline-formula>-estimation error with respect to <inline-formula id="S2.p1.m5"><tex-math>\theta^{\natural}</tex-math></inline-formula>.
A common limitation of <xref ref-type="bibr" rid="Koltchinskii2013a">[Koltchinskii2013a, Mendelson2014, Oymak2013, Oymak2013a, Vershynin2014]</xref> is that the results are not extendable to general non-linear statistical models.</p><p>Another research direction considers constrained estimation in possibly non-linear statistical models <xref ref-type="bibr" rid="Plan2013a">[Plan2013a, Plan2015, Plan2014a]</xref>.
A constrained <inline-formula id="S2.p2.m1"><tex-math>M</tex-math></inline-formula>-estimator for logistic regression was proposed and analyzed in <xref ref-type="bibr" rid="Plan2013a">[Plan2013a]</xref>.
In <xref ref-type="bibr" rid="Plan2014a">[Plan2014a]</xref>, the authors proposed and analyzed a universal projection-based estimator for regression in generalized linear models (GLMs).
In <xref ref-type="bibr" rid="Plan2015">[Plan2015]</xref>, the authors analyzed the performance of the constrained LS estimator in GLMs.
A common limitation of <xref ref-type="bibr" rid="Plan2013a">[Plan2013a, Plan2015, Plan2014a]</xref> is that the results are valid only for the specific proposed estimators, and they do not even apply to the constrained maximum-likelihood (ML) estimator, which is the most popular approach in practice. Moreover, the proposed estimators in <xref ref-type="bibr" rid="Plan2013a">[Plan2013a, Plan2015, Plan2014a]</xref> can only recover the true parameter up to a scale ambiguity.
</p><p>We say that the constraint is <italic>matched</italic> if <inline-formula id="S2.p3.m1"><tex-math>\theta^{\natural}</tex-math></inline-formula> lies on the boundary of <inline-formula id="S2.p3.m2"><tex-math>\mathcal{G}</tex-math></inline-formula> in (<xref ref-type="labelref" rid="LABEL:eq_that">1</xref>) (or <inline-formula id="S2.p3.m3"><tex-math>c=g(\theta^{\natural})</tex-math></inline-formula>), and <italic>mismatched</italic> if <inline-formula id="S2.p3.m4"><tex-math>\theta^{\natural}</tex-math></inline-formula> lies strictly in <inline-formula id="S2.p3.m5"><tex-math>\mathcal{G}</tex-math></inline-formula> (or <inline-formula id="S2.p3.m6"><tex-math>c&lt;g(\theta^{\natural})</tex-math></inline-formula>). The analyses in <xref ref-type="bibr" rid="Oymak2013a">[Oymak2013a, Oymak2013]</xref> require the constraint to be matched, while in practice the exact value of <inline-formula id="S2.p3.m7"><tex-math>g(\theta^{\natural})</tex-math></inline-formula> is seldom known. The constraint in <xref ref-type="bibr" rid="Koltchinskii2013a">[Koltchinskii2013a]</xref> is always matched due to the special structure of quantum density operators. The error bounds in <xref ref-type="bibr" rid="Plan2013a">[Plan2013a, Vershynin2014]</xref> can be overly pessimistic, because they hold for all <inline-formula id="S2.p3.m8"><tex-math>\theta^{\natural}\in\mathcal{G}</tex-math></inline-formula>. The results in <xref ref-type="bibr" rid="Mendelson2014">[Mendelson2014, Plan2015, Plan2014a]</xref> do not require a matched constraint and depend on <inline-formula id="S2.p3.m9"><tex-math>\theta^{\natural}</tex-math></inline-formula>; our result is of this kind. Recall that, however, <xref ref-type="bibr" rid="Mendelson2014">[Mendelson2014]</xref> is limited to specific statistical models, and <xref ref-type="bibr" rid="Plan2015">[Plan2015, Plan2014a]</xref> are limited to specific <inline-formula id="S2.p3.m10"><tex-math>M</tex-math></inline-formula>-estimators.</p></sec><sec id="LABEL:sec_framework"><title>A Geometric Framework</title><sec id="LABEL:sec_basic_idea"><title>Basic Idea</title><p>To illustrate the basic idea of our framework, let us start with a simple setting, where <inline-formula id="S3.SS1.p1.m1"><tex-math>f_{n}</tex-math></inline-formula> is strongly convex with parameter <inline-formula id="S3.SS1.p1.m2"><tex-math>\mu&gt;0</tex-math></inline-formula>, i.e.,</p><p><disp-formula id="S3.Ex1"><tex-math>\left\langle\nabla f_{n}(y)-\nabla f_{n}(x),y-x\right\rangle\geq\mu\left\|y-x%
\right\|_{2}^{2},</tex-math></disp-formula></p><p>for any <inline-formula id="S3.SS1.p1.m3"><tex-math>x,y\in\mathrm{dom}\,f</tex-math></inline-formula>. Note that then <inline-formula id="S3.SS1.p1.m4"><tex-math>\hat{\theta}</tex-math></inline-formula> is uniquely defined.</p><p>Define <inline-formula id="S3.SS1.p2.m1"><tex-math>\iota_{g}:\mathbb{R}^{p}\to\mathbb{R}\cup\left\{+\infty\right\}</tex-math></inline-formula> as the indicator function of the constraint set <inline-formula id="S3.SS1.p2.m2"><tex-math>\mathcal{G}</tex-math></inline-formula>; that is, <inline-formula id="S3.SS1.p2.m3"><tex-math>\iota_{\mathcal{G}}(\theta)=0</tex-math></inline-formula> if <inline-formula id="S3.SS1.p2.m4"><tex-math>\theta\in\mathcal{G}</tex-math></inline-formula>, and <inline-formula id="S3.SS1.p2.m5"><tex-math>\iota_{\mathcal{G}}(\theta)=+\infty</tex-math></inline-formula> otherwise.
By the strong convexity of <inline-formula id="S3.SS1.p2.m6"><tex-math>f_{n}</tex-math></inline-formula>, we have</p><p><disp-formula id="LABEL:eq_strong_convexity"><tex-math>\left\langle\nabla f_{n}(\hat{\theta})-\nabla f_{n}(\theta^{\natural}),\hat{%
\theta}-\theta^{\natural}\right\rangle\geq\mu\left\|\hat{\theta}-\theta^{%
\natural}\right\|_{2}^{2}.</tex-math></disp-formula></p><p>By the convexity of <inline-formula id="S3.SS1.p2.m7"><tex-math>\iota_{g}</tex-math></inline-formula>, or the monotonicity of the subdifferential mapping, we have</p><p><disp-formula id="LABEL:eq_monotonicity"><tex-math>\left\langle\hat{z}-z^{\natural},\hat{\theta}-\theta^{\natural}\right\rangle%
\geq 0,</tex-math></disp-formula></p><p>for any <inline-formula id="S3.SS1.p2.m8"><tex-math>\hat{z}\in\partial\iota_{g}(\hat{\theta})</tex-math></inline-formula>, and any <inline-formula id="S3.SS1.p2.m9"><tex-math>z^{\natural}\in\partial\iota_{g}(\theta^{\natural})</tex-math></inline-formula>. Summing up (<xref ref-type="labelref" rid="LABEL:eq_strong_convexity">5</xref>) and (<xref ref-type="labelref" rid="LABEL:eq_monotonicity">6</xref>), we obtain</p><p><disp-formula id="S3.Ex2"><tex-math>\left\langle\nabla f_{n}(\hat{\theta})+\hat{z}-\nabla f_{n}(\theta^{\natural})%
-z^{\natural},\hat{\theta}-\theta^{\natural}\right\rangle\geq\mu\left\|\hat{%
\theta}-\theta^{\natural}\right\|_{2}^{2},</tex-math></disp-formula></p><p>for any <inline-formula id="S3.SS1.p2.m10"><tex-math>\hat{z}\in\partial\iota_{g}(\hat{\theta})</tex-math></inline-formula>. By the optimality condition of <inline-formula id="S3.SS1.p2.m11"><tex-math>\hat{\theta}</tex-math></inline-formula>, there exists some <inline-formula id="S3.SS1.p2.m12"><tex-math>\hat{z}\in\partial\iota_{\mathcal{G}}(\hat{\theta})</tex-math></inline-formula> such that</p><p><disp-formula id="LABEL:eq_optimality"><tex-math>0=\nabla f_{n}(\hat{\theta})+\hat{z},</tex-math></disp-formula></p><p>and hence we have</p><p><disp-formula id="S3.Ex3"><tex-math>\left\langle-\nabla f_{n}(\theta^{\natural})-z^{\natural},\hat{\theta}-\theta^%
{\natural}\right\rangle\geq\mu\left\|\hat{\theta}-\theta^{\natural}\right\|_{2%
}^{2},</tex-math></disp-formula></p><p>for any <inline-formula id="S3.SS1.p2.m13"><tex-math>z^{\natural}\in\partial\iota_{g}(\theta^{\natural})</tex-math></inline-formula>. Since <inline-formula id="S3.SS1.p2.m14"><tex-math>\partial\iota_{g}(\theta^{\natural})</tex-math></inline-formula> is always a closed convex cone, we can choose <inline-formula id="S3.SS1.p2.m15"><tex-math>z^{\natural}=0</tex-math></inline-formula> and obtain</p><p><disp-formula id="LABEL:eq_bad_lower"><tex-math>\left\langle-\nabla f_{n}(\theta^{\natural}),\hat{\theta}-\theta^{\natural}%
\right\rangle\geq\mu\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}^{2}.</tex-math></disp-formula></p><p>Applying the Cauchy-Schwarz inequality to the left-hand side, we obtain</p><disp-formula-group id="A5.EGx1"><disp-formula id="S3.Ex4"><tex-math/></disp-formula></disp-formula-group><p>or</p><p><disp-formula id="S3.E9"><tex-math>\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq\frac{1}{\mu}\left\|%
\nabla f_{n}(\theta^{\natural})\right\|_{2}.</tex-math></disp-formula></p><p>Taking expectations on both sides, we immediately obtain the following estimation error bound:</p><p><disp-formula id="LABEL:eq_bad_bound"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq\frac{1}{\mu}%
\mathbb{E}\,\left\|\nabla f_{n}(\theta^{\natural})\right\|_{2}.</tex-math></disp-formula></p><p>The gradient at the true parameter, <inline-formula id="S3.SS1.p2.m16"><tex-math>\nabla f_{n}(\theta^{\natural})</tex-math></inline-formula>, usually concentrates around <inline-formula id="S3.SS1.p2.m17"><tex-math>0</tex-math></inline-formula> with high probability.</p><p>The simple error bound (<xref ref-type="labelref" rid="LABEL:eq_bad_bound">10</xref>) is not desirable for two reasons:
</p><list list-type="ordered" id="I2"><list-item id="I2.i1"><p>In the high-dimensional setting where <inline-formula id="I2.i1.p1.m1"><tex-math>n&lt;p</tex-math></inline-formula>, <inline-formula id="I2.i1.p1.m2"><tex-math>f_{n}</tex-math></inline-formula> cannot be strongly convex even for the basic LS estimator.</p></list-item><list-item id="I2.i2"><p>It does not depend on the choice of <inline-formula id="I2.i2.p1.m1"><tex-math>g</tex-math></inline-formula>.</p></list-item></list><p>We address the first issue in Section <xref ref-type="labelref" rid="LABEL:sec_RSC">3.2</xref>, and the second issue in Section <xref ref-type="labelref" rid="LABEL:sec_refined_bound">3.3</xref>.</p></sec><sec id="LABEL:sec_RSC"><title>Restricted Strong Convexity</title><p>Note that in order to facilitate the arguments in the previous sub-section, we only require (<xref ref-type="labelref" rid="LABEL:eq_strong_convexity">5</xref>) to hold for <inline-formula id="S3.SS2.p1.m1"><tex-math>\hat{\theta}</tex-math></inline-formula> and <inline-formula id="S3.SS2.p1.m2"><tex-math>\theta^{\natural}</tex-math></inline-formula>, instead of any two vectors in <inline-formula id="S3.SS2.p1.m3"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>. Therefore, we only need <inline-formula id="S3.SS2.p1.m4"><tex-math>f_{n}</tex-math></inline-formula> to satisfy some <italic>restricted</italic> notion of strong convexity. Similar (but not exactly the same) ideas had appeared in <xref ref-type="bibr" rid="Chandrasekaran2012">[Chandrasekaran2012, Negahban2012]</xref>, and can be traced back to <xref ref-type="bibr" rid="Bickel2009">[Bickel2009, Geer2007]</xref>.</p><statement id="S3.Thmdefn1"><title> (Feasible Set and Feasible Cone).</title><p>The <italic>feasible set</italic> of <inline-formula id="S3.Thmdefn1.p1.m1"><tex-math>g</tex-math></inline-formula> at <inline-formula id="S3.Thmdefn1.p1.m2"><tex-math>\theta^{\natural}</tex-math></inline-formula>, denoted by <inline-formula id="S3.Thmdefn1.p1.m3"><tex-math>\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula>, is given by</p><p><disp-formula id="S3.Ex5"><tex-math>\mathcal{F}_{g}(\theta^{\natural}):=\mathcal{G}-\theta^{\natural}=\left\{%
\theta-\theta^{\natural}:\theta\in\mathcal{G}\right\}.</tex-math></disp-formula></p><p>The <italic>feasible cone</italic> of <inline-formula id="S3.Thmdefn1.p1.m4"><tex-math>g</tex-math></inline-formula> at <inline-formula id="S3.Thmdefn1.p1.m5"><tex-math>\theta^{\natural}</tex-math></inline-formula>, denoted by <inline-formula id="S3.Thmdefn1.p1.m6"><tex-math>\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula>, is defined as the conic hull of <inline-formula id="S3.Thmdefn1.p1.m7"><tex-math>\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula>.</p></statement><p>By the definition of <inline-formula id="S3.SS2.p2.m1"><tex-math>\hat{\theta}</tex-math></inline-formula>, the estimation error must satisfy <inline-formula id="S3.SS2.p2.m2"><tex-math>\hat{\theta}-\theta^{\natural}\in\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula>.</p><statement id="LABEL:def_RSC"><title> (Restricted Strong Convexity).</title><p>The function <inline-formula id="S3.Thmdefn2.p1.m1"><tex-math>f_{n}</tex-math></inline-formula> satisfies the restricted strong convexity (RSC) condition with parameter <inline-formula id="S3.Thmdefn2.p1.m2"><tex-math>\mu&gt;0</tex-math></inline-formula> if</p><p><disp-formula id="LABEL:eq_RSC"><tex-math>\left\langle\nabla f_{n}(\theta^{\natural}+e)-\nabla f_{n}(\theta^{\natural}),%
e\right\rangle\geq\mu\left\|e\right\|_{2}^{2},</tex-math></disp-formula></p><p>for any <inline-formula id="S3.Thmdefn2.p1.m3"><tex-math>e\in\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula>.</p></statement><p>If <inline-formula id="S3.SS2.p3.m1"><tex-math>f_{n}</tex-math></inline-formula> is twice continuously differentiable, we have a sufficient condition.</p><statement id="LABEL:prop_RSC_hessian"><title>.</title><p><italic>The function <inline-formula id="S3.Thmthm1.p1.m1"><tex-math>f_{n}</tex-math></inline-formula> satisfies the RSC condition with parameter <inline-formula id="S3.Thmthm1.p1.m2"><tex-math>\mu&gt;0</tex-math></inline-formula> if</italic></p><p><disp-formula id="S3.Ex6"><tex-math>\left\langle e,\nabla^{2}f_{n}(\theta^{\natural}+\lambda e)e\right\rangle\geq%
\mu\left\|e\right\|_{2}^{2},</tex-math></disp-formula></p><p><italic>for all <inline-formula id="S3.Thmthm1.p1.m3"><tex-math>\lambda\in[0,1]</tex-math></inline-formula> and all <inline-formula id="S3.Thmthm1.p1.m4"><tex-math>e\in\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula>.</italic></p></statement><p>The uniqueness of <inline-formula id="S3.SS2.p4.m1"><tex-math>\hat{\theta}</tex-math></inline-formula> and the derivation of the error bound in Section <xref ref-type="labelref" rid="LABEL:sec_basic_idea">3.1</xref> are still valid even when <inline-formula id="S3.SS2.p4.m2"><tex-math>n&lt;p</tex-math></inline-formula>, as long as <inline-formula id="S3.SS2.p4.m3"><tex-math>f_{n}</tex-math></inline-formula> satisfies the RSC condition with some parameter <inline-formula id="S3.SS2.p4.m4"><tex-math>\mu&gt;0</tex-math></inline-formula>.</p></sec><sec id="LABEL:sec_refined_bound"><title>Refined Error Bound</title><p>We address the dependence of the estimation error on the choice of <inline-formula id="S3.SS3.p1.m1"><tex-math>g</tex-math></inline-formula>, and derive a refined error bound in this sub-section.</p><p>We note that</p><p><disp-formula id="S3.Ex7"><tex-math>\left\langle-\nabla f_{n}(\theta^{\natural}),\hat{\theta}-\theta^{\natural}%
\right\rangle=\left\|\Pi_{\overline{\hat{\theta}-\theta^{\natural}}}\left(-%
\nabla f_{n}(\theta^{\natural})\right)\right\|_{2}\left\|\hat{\theta}-\theta^{%
\natural}\right\|_{2},</tex-math></disp-formula></p><p>where <inline-formula id="S3.SS3.p2.m1"><tex-math>\Pi_{\overline{\hat{\theta}-\theta^{\natural}}}(\cdot)</tex-math></inline-formula> denotes the projection onto the conic hull of <inline-formula id="S3.SS3.p2.m2"><tex-math>\left\{\hat{\theta}-\theta^{\natural}\right\}</tex-math></inline-formula> (which is a half-line or <inline-formula id="S3.SS3.p2.m3"><tex-math>\{0\}</tex-math></inline-formula>). This implies, by (<xref ref-type="labelref" rid="LABEL:eq_bad_lower">8</xref>),
</p><p><disp-formula id="S3.Ex8"><tex-math>\left\|\Pi_{\overline{\hat{\theta}-\theta^{\natural}}}\left(-\nabla f_{n}(%
\theta^{\natural})\right)\right\|_{2}\geq\mu\left\|\hat{\theta}-\theta^{%
\natural}\right\|_{2}.</tex-math></disp-formula></p><p>The left-hand side, however, is not tractable due to its dependence on <inline-formula id="S3.SS3.p2.m4"><tex-math>\hat{\theta}</tex-math></inline-formula>. As <inline-formula id="S3.SS3.p2.m5"><tex-math>\hat{\theta}-\theta^{\natural}\in\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula> by definition, we consider a looser bound:</p><p><disp-formula id="LABEL:eq_concentration"><tex-math>\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}(-\nabla f_{n}(%
\theta^{\natural}))\right\|_{2}\geq\mu\left\|\hat{\theta}-\theta^{\natural}%
\right\|_{2},</tex-math></disp-formula></p><p>where <inline-formula id="S3.SS3.p2.m6"><tex-math>\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}(\cdot)</tex-math></inline-formula> denotes projection onto the feasible cone <inline-formula id="S3.SS3.p2.m7"><tex-math>\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula>.</p><p>Taking expectations on both sides, we obtain the following lemma.</p><statement id="LABEL:lem_fundamental"><title>.</title><p><italic>Assume that <inline-formula id="S3.Thmthm2.p1.m1"><tex-math>f_{n}</tex-math></inline-formula> satisfies the RSC condition with parameter <inline-formula id="S3.Thmthm2.p1.m2"><tex-math>\mu&gt;0</tex-math></inline-formula>. Then <inline-formula id="S3.Thmthm2.p1.m3"><tex-math>\hat{\theta}</tex-math></inline-formula> is uniquely defined, and satisfies</italic></p><p><disp-formula id="S3.Ex9"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq\frac{1}{\mu}%
\mathbb{E}\,\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}(-\nabla
f%
_{n}(\theta^{\natural}))\right\|_{2}.</tex-math></disp-formula></p></statement><p>Since <inline-formula id="S3.SS3.p4.m1"><tex-math>-\nabla f_{n}(\theta^{\natural})</tex-math></inline-formula> is a descent direction of <inline-formula id="S3.SS3.p4.m2"><tex-math>f_{n}</tex-math></inline-formula>, if its direction is coherent with the feasible cone <inline-formula id="S3.SS3.p4.m3"><tex-math>\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula>, we may find some point <inline-formula id="S3.SS3.p4.m4"><tex-math>\hat{\theta}^{\prime}</tex-math></inline-formula> far away from <inline-formula id="S3.SS3.p4.m5"><tex-math>\theta^{\natural}</tex-math></inline-formula> in the feasible set <inline-formula id="S3.SS3.p4.m6"><tex-math>\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula> such that <inline-formula id="S3.SS3.p4.m7"><tex-math>f_{n}(\hat{\theta}^{\prime})</tex-math></inline-formula> is much smaller than <inline-formula id="S3.SS3.p4.m8"><tex-math>f_{n}(\theta^{\natural})</tex-math></inline-formula>, and hence the estimation error can be large. This provides an intuitive interpretation of the lemma.</p><p>Since projection onto a closed convex set is a non-expansive mapping, we have</p><p><disp-formula id="S3.Ex10"><tex-math>\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}(-\nabla f_{n}(%
\theta^{\natural}))\right\|_{2}\leq\left\|\nabla f_{n}(\theta^{\natural})%
\right\|_{2},</tex-math></disp-formula></p><p>so the error bound is always no larger than the one in Section <xref ref-type="labelref" rid="LABEL:sec_basic_idea">3.1</xref>.</p><p>Lemma <xref ref-type="labelref" rid="LABEL:lem_fundamental">3.2</xref> is the theoretical foundation of the rest of this paper.</p></sec></sec><sec id="LABEL:sec_matched"><title>Estimation Error Bound in Terms of the Gaussian Width</title><p>We apply Lemma <xref ref-type="labelref" rid="LABEL:lem_fundamental">3.2</xref> to constrained ML estimators in a GLM with the canonical link function. Examples of a canonical GLM include the Gaussian linear, logistic, gamma, and Poisson regression models.</p><p>Let <inline-formula id="S4.p2.m1"><tex-math>\theta^{\natural}\in\mathbb{R}^{p}</tex-math></inline-formula> be the parameter to be estimated, or the unknown vector of regression coefficients. In a canonical GLM, the negative log-likelihood of a sample <inline-formula id="S4.p2.m2"><tex-math>y</tex-math></inline-formula>, given <inline-formula id="S4.p2.m3"><tex-math>\theta^{\natural}</tex-math></inline-formula>, is of the form (up to scaling and shifting by some constants)</p><p><disp-formula id="S4.Ex11"><tex-math>L(y;\theta^{\natural})=y\left\langle a_{i},\theta^{\natural}\right\rangle-b(%
\left\langle a_{i},\theta^{\natural}\right\rangle),</tex-math></disp-formula></p><p>where <inline-formula id="S4.p2.m4"><tex-math>a_{1},\ldots,a_{n}\in\mathbb{R}^{p}</tex-math></inline-formula> are given, and we assume that <inline-formula id="S4.p2.m5"><tex-math>b</tex-math></inline-formula> is some given concave function. Let <inline-formula id="S4.p2.m6"><tex-math>(y_{1},\ldots,y_{n})\in\mathbb{R}^{n}</tex-math></inline-formula> be the sample. The constrained ML estimator is given by (<xref ref-type="labelref" rid="LABEL:eq_that">1</xref>) with</p><p><disp-formula id="LABEL:eq_fn_glm"><tex-math>f_{n}(\theta):=\frac{1}{n}\sum_{i=1}^{n}L(y_{i},\theta),</tex-math></disp-formula></p><p>and <inline-formula id="S4.p2.m7"><tex-math>g</tex-math></inline-formula> being some continuous convex function. For simplicity, we consider the case where <inline-formula id="S4.p2.m8"><tex-math>c=g(\theta^{\natural})</tex-math></inline-formula> in this section; we address the case where <inline-formula id="S4.p2.m9"><tex-math>c&gt;g(\theta^{\natural})</tex-math></inline-formula> in Section <xref ref-type="labelref" rid="LABEL:sec_mismatch">5</xref>.</p><p>We specialize Lemma <xref ref-type="labelref" rid="LABEL:lem_fundamental">3.2</xref> to the canonical GLM and obtain the following theorem.</p><statement id="LABEL:def_gwidth"><title> (Gaussian width <xref ref-type="bibr" rid="Chandrasekaran2012">[Chandrasekaran2012, Mendelson2007, Tropp2014]</xref>).</title><p>Let <inline-formula id="S4.Thmdefn1.p1.m1"><tex-math>\mathcal{C}\subseteq\mathbb{R}^{p}</tex-math></inline-formula>. The <italic>Gaussian width</italic> of <inline-formula id="S4.Thmdefn1.p1.m2"><tex-math>\mathcal{C}</tex-math></inline-formula> is given by</p><p><disp-formula id="S4.Ex12"><tex-math>\omega_{t}(\mathcal{C}):=\mathbb{E}\,\sup_{v\in\mathcal{C}\cap t\mathcal{S}^{p%
-1}}\left\{\left\langle h,v\right\rangle\right\},</tex-math></disp-formula></p><p>where <inline-formula id="S4.Thmdefn1.p1.m3"><tex-math>h:=(h_{1},\ldots,h_{p})</tex-math></inline-formula> is a vector of i.i.d. standard Gaussian random variables, and <inline-formula id="S4.Thmdefn1.p1.m4"><tex-math>\mathcal{S}^{p-1}</tex-math></inline-formula> denotes the unit <inline-formula id="S4.Thmdefn1.p1.m5"><tex-math>\ell_{2}</tex-math></inline-formula>-sphere in <inline-formula id="S4.Thmdefn1.p1.m6"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>.</p></statement><statement id="LABEL:thm_no_mismatch"><title>.</title><p><italic>Consider the canonical GLM and the corresponding ML estimator described above for <inline-formula id="S4.Thmthm1.p1.m1"><tex-math>c=g(\theta^{\natural})</tex-math></inline-formula>.
Assume that the entries of <inline-formula id="S4.Thmthm1.p1.m2"><tex-math>a_{1},\ldots,a_{n}</tex-math></inline-formula> are either all i.i.d. standard Gaussian or all i.i.d. Rademacher random variables (random variables taking values in <inline-formula id="S4.Thmthm1.p1.m3"><tex-math>\left\{+1,-1\right\}</tex-math></inline-formula> with equal probability), and <inline-formula id="S4.Thmthm1.p1.m4"><tex-math>f_{n}</tex-math></inline-formula> satisfies the RSC condition for <inline-formula id="S4.Thmthm1.p1.m5"><tex-math>\mu&gt;0</tex-math></inline-formula> with probability at least <inline-formula id="S4.Thmthm1.p1.m6"><tex-math>1/2</tex-math></inline-formula>.
Then</italic></p><p><disp-formula id="S4.Ex13"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq 2\sqrt{2\pi}%
\,\sigma_{\max}\frac{\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{\natural})})%
}{\mu\sqrt{n}},</tex-math></disp-formula></p><p><italic>where <inline-formula id="S4.Thmthm1.p1.m7"><tex-math>\sigma_{\max}:=\max_{i}\sqrt{\mathrm{var}\,y_{i}}</tex-math></inline-formula>.</italic></p></statement><statement id="Thmremx1"><title>.</title><p>Note that the expectation is with respect to <inline-formula id="Thmremx1.p1.m1"><tex-math>A</tex-math></inline-formula> and <inline-formula id="Thmremx1.p1.m2"><tex-math>\varepsilon</tex-math></inline-formula>, conditioned on the event that the RSC condition holds.</p></statement><p>The feasible cone <inline-formula id="S4.p4.m1"><tex-math>\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula> coincides with the tangent cone of <inline-formula id="S4.p4.m2"><tex-math>g</tex-math></inline-formula> at <inline-formula id="S4.p4.m3"><tex-math>\theta^{\natural}</tex-math></inline-formula> defined in <xref ref-type="bibr" rid="Chandrasekaran2012">[Chandrasekaran2012]</xref>. Therefore, to evaluate the estimation error bound, we only need to evaluate the Gaussian width of the corresponding tangent cone. We note that there are already many results for a variety of commonly used regularization functions, such as the <inline-formula id="S4.p4.m4"><tex-math>\ell_{1}</tex-math></inline-formula>-norm, nuclear norm, total variation semi-norm, and general atomic norms <xref ref-type="bibr" rid="Cai2013">[Cai2013, Chandrasekaran2012, Foygel2014, Plan2013a, Rao2012, Vershynin2014]</xref>.
Therefore, for most of the applications, we only need to <italic>plug in</italic> an existing bound on the Gaussian width.</p><p>Finally, we would like to emphasize that the Gaussian width in Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref> comes from bounding the random process induced by the random gradient <inline-formula id="S4.p5.m1"><tex-math>\nabla f_{n}(\theta^{\natural})</tex-math></inline-formula> (cf. the proof of Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref>), instead of being a consequence of applying Gordon’s Lemma. That is, our result is essentially different from those in <xref ref-type="bibr" rid="Chandrasekaran2012">[Chandrasekaran2012, Oymak2013a, Oymak2013]</xref>.</p></sec><sec id="LABEL:sec_mismatch"><title>Effect of a Mismatched Constraint</title><p>In this section, we discuss the effect of a mismatched constraint for ML regression in a canonical GLM. Recall that the constraint set <inline-formula id="S5.p1.m1"><tex-math>\mathcal{G}</tex-math></inline-formula> is called <italic>mismatched</italic> if <inline-formula id="S5.p1.m2"><tex-math>c&gt;g(\theta^{\natural})</tex-math></inline-formula> in (<xref ref-type="labelref" rid="LABEL:eq_that">1</xref>).</p><p>The notion of the RSC in Definition <xref ref-type="labelref" rid="LABEL:def_RSC">3.2</xref> is no longer meaningful when the constraint set is mismatched. Take ML regression in the Gaussian linear model for example, for which the corresponding <inline-formula id="S5.p2.m1"><tex-math>f_{n}</tex-math></inline-formula> is given in (<xref ref-type="labelref" rid="LABEL:eq_LS">2</xref>). Let <inline-formula id="S5.p2.m2"><tex-math>A\in\mathbb{R}^{n\times p}</tex-math></inline-formula> be defined as in Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref>.
The RSC condition requires</p><p><disp-formula id="S5.Ex14"><tex-math>\left\langle\nabla f_{n}(\theta^{\natural}+e)-\nabla f_{n}(\theta^{\natural}),%
e\right\rangle=\frac{1}{n}\left\|Ae\right\|_{2}^{2}\geq\mu\left\|e\right\|_{2}%
^{2},</tex-math></disp-formula></p><p>for some <inline-formula id="S5.p2.m3"><tex-math>\mu&gt;0</tex-math></inline-formula> and all <inline-formula id="S5.p2.m4"><tex-math>e\in\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula>, where we say <inline-formula id="S5.p2.m5"><tex-math>e\in\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula> instead of <inline-formula id="S5.p2.m6"><tex-math>e\in\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula> because <inline-formula id="S5.p2.m7"><tex-math>A</tex-math></inline-formula> is a linear operator. Since when the constraint is mismatched, <inline-formula id="S5.p2.m8"><tex-math>\overline{\mathcal{F}_{g}(\theta^{\natural})}</tex-math></inline-formula> is the whole space <inline-formula id="S5.p2.m9"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>, the RSC condition requires <inline-formula id="S5.p2.m10"><tex-math>A</tex-math></inline-formula> to be a non-singular matrix. This cannot be true in the high-dimensional setting, where <inline-formula id="S5.p2.m11"><tex-math>A\in\mathbb{R}^{n\times p}</tex-math></inline-formula> and <inline-formula id="S5.p2.m12"><tex-math>n&lt;p</tex-math></inline-formula>.</p><p><bold>Our Approach: </bold> Let <inline-formula id="S5.p3.m1"><tex-math>t&gt;0</tex-math></inline-formula> and denote by <inline-formula id="S5.p3.m2"><tex-math>\mathcal{B}</tex-math></inline-formula> the unit <inline-formula id="S5.p3.m3"><tex-math>\ell_{2}</tex-math></inline-formula>-ball in <inline-formula id="S5.p3.m4"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>. We partition the feasible set <inline-formula id="S5.p3.m5"><tex-math>\mathcal{F}_{g}(\theta^{\natural})</tex-math></inline-formula> as</p><p><disp-formula id="S5.Ex15"><tex-math>\mathcal{F}_{g}(\theta^{\natural})=(\mathcal{F}_{g}(\theta^{\natural})\cap t%
\mathcal{B})\cup(\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}).</tex-math></disp-formula></p><p>When <inline-formula id="S5.p3.m6"><tex-math>t</tex-math></inline-formula> is large enough, the conic hull of <inline-formula id="S5.p3.m7"><tex-math>(\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B})</tex-math></inline-formula> will not be the whole space <inline-formula id="S5.p3.m8"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>, so it is possible to have restricted strong convexity on <inline-formula id="S5.p3.m9"><tex-math>(\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B})</tex-math></inline-formula> when <inline-formula id="S5.p3.m10"><tex-math>n&lt;p</tex-math></inline-formula>. If the error vector <inline-formula id="S5.p3.m11"><tex-math>\hat{\theta}-\theta^{\natural}</tex-math></inline-formula> lies in <inline-formula id="S5.p3.m12"><tex-math>(\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B})</tex-math></inline-formula>, we can obtain an error bound, say, <inline-formula id="S5.p3.m13"><tex-math>\tilde{t}</tex-math></inline-formula>, as in Section <xref ref-type="labelref" rid="LABEL:sec_matched">4</xref>; otherwise, if the error vector lies in <inline-formula id="S5.p3.m14"><tex-math>\mathcal{B}_{t}</tex-math></inline-formula>, a naïve error bound is the radius of the ball, i.e., <inline-formula id="S5.p3.m15"><tex-math>t</tex-math></inline-formula>. Finally, we can bound the estimation error from above by the maximum of <inline-formula id="S5.p3.m16"><tex-math>\tilde{t}</tex-math></inline-formula> and <inline-formula id="S5.p3.m17"><tex-math>t</tex-math></inline-formula>. Note that <inline-formula id="S5.p3.m18"><tex-math>\tilde{t}</tex-math></inline-formula> is implicitly dependent on <inline-formula id="S5.p3.m19"><tex-math>t</tex-math></inline-formula>.
</p><p>The arguments in the previous paragraph can be made precise as in Lemma <xref ref-type="labelref" rid="LABEL:lem_mismatched">5.1</xref>, which is an analogue of Lemma <xref ref-type="labelref" rid="LABEL:lem_fundamental">3.2</xref> in the mismatched case. Lemma <xref ref-type="labelref" rid="LABEL:lem_mismatched">5.1</xref> holds for arbitrary constrained <inline-formula id="S5.p4.m1"><tex-math>M</tex-math></inline-formula>-estimators of the form (<xref ref-type="labelref" rid="LABEL:eq_that">1</xref>) and statistical models.</p><statement id="LABEL:lem_mismatched"><title>.</title><p><italic>Suppose that for some <inline-formula id="S5.Thmthm1.p1.m1"><tex-math>t&gt;0</tex-math></inline-formula>, we have</italic></p><disp-formula-group id="A5.EGx2"><disp-formula id="LABEL:eq_RRSC"><tex-math>\displaystyle\left\langle\nabla f_{n}(\theta^{\natural}+e)-\nabla f_{n}(\theta%
^{\natural}),e\right\rangle\geq\mu\left\|e\right\|_{2}^{2},</tex-math></disp-formula></disp-formula-group><p><italic>for some <inline-formula id="S5.Thmthm1.p1.m2"><tex-math>\mu&gt;0</tex-math></inline-formula> and all <inline-formula id="S5.Thmthm1.p1.m3"><tex-math>e\in\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}</tex-math></inline-formula>. Then</italic></p><p><disp-formula id="S5.Ex16"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq t+\mathbb{E}%
\,\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{%
B}}}\left(-\nabla f_{n}(\theta^{\natural})\right)\right\|_{2}.</tex-math></disp-formula></p></statement><p>We can also prove an analogue of Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref> for constrained ML regression in a canonical GLM.</p><statement id="LABEL:cor_mismatched"><title>.</title><p><italic>Consider the canonical GLM and the corresponding ML estimator described in Section <xref ref-type="labelref" rid="LABEL:sec_matched">4</xref>, for <inline-formula id="S5.Thmthm2.p1.m1"><tex-math>c&gt;g(\theta^{\natural})</tex-math></inline-formula>. Let <inline-formula id="S5.Thmthm2.p1.m2"><tex-math>A</tex-math></inline-formula> be defined as in Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref> and let <inline-formula id="S5.Thmthm2.p1.m3"><tex-math>t&gt;0</tex-math></inline-formula>. Suppose that (<xref ref-type="labelref" rid="LABEL:eq_RRSC">14</xref>) holds true with for some <inline-formula id="S5.Thmthm2.p1.m4"><tex-math>\mu&gt;0</tex-math></inline-formula> with probability at least <inline-formula id="S5.Thmthm2.p1.m5"><tex-math>1/2</tex-math></inline-formula>.
Then we have</italic></p><p><disp-formula id="S5.Ex17"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq t+2\sqrt{2%
\pi}\,\sigma_{\max}\frac{\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{\natural%
})\setminus t\mathcal{B}})}{\mu\sqrt{n}},</tex-math></disp-formula></p><p><italic>where <inline-formula id="S5.Thmthm2.p1.m6"><tex-math>\sigma_{\max}</tex-math></inline-formula> is defined as in Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref>.</italic></p></statement><p>The proofs of Lemma <xref ref-type="labelref" rid="LABEL:lem_mismatched">5.1</xref> and Corollary <xref ref-type="labelref" rid="LABEL:cor_mismatched">5.2</xref> are similar to the proofs of Lemma <xref ref-type="labelref" rid="LABEL:lem_fundamental">3.2</xref> and Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref>, respectively.</p></sec><sec id="LABEL:sec_appl"><title>Applications</title><p>Once the conditions (<xref ref-type="labelref" rid="LABEL:eq_RSC">11</xref>) and (<xref ref-type="labelref" rid="LABEL:eq_RRSC">14</xref>) are verified, our results Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref> and Corollary <xref ref-type="labelref" rid="LABEL:cor_mismatched">5.2</xref> immediately follow. We explicitly verify the conditions for two applications and obtain the corresponding estimation error bounds.</p><p>The first application is regression by the constrained LS estimator in a Gaussian linear model. Let <inline-formula id="S6.p2.m1"><tex-math>\theta^{\natural}\in\mathbb{R}^{p}</tex-math></inline-formula> and <inline-formula id="S6.p2.m2"><tex-math>a_{1},\ldots,a_{n}</tex-math></inline-formula> be vectors in <inline-formula id="S6.p2.m3"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>. The sample is given by</p><p><disp-formula id="S6.Ex18"><tex-math>y_{i}=\langle a_{i},\theta^{\natural}\rangle+\sigma w_{i},\quad i=1,\ldots,n,</tex-math></disp-formula></p><p>for some <inline-formula id="S6.p2.m4"><tex-math>\sigma&gt;0</tex-math></inline-formula>, where <inline-formula id="S6.p2.m5"><tex-math>w_{1},\ldots,w_{n}</tex-math></inline-formula> are i.i.d. standard Gaussian random variables. We consider the constrained LS estimator, for which <inline-formula id="S6.p2.m6"><tex-math>f_{n}</tex-math></inline-formula> is given by (<xref ref-type="labelref" rid="LABEL:eq_LS">2</xref>), and <inline-formula id="S6.p2.m7"><tex-math>\mathcal{G}:=\left\{\theta:g(\theta)\leq c\right\}</tex-math></inline-formula> for some <inline-formula id="S6.p2.m8"><tex-math>c\geq g(\theta^{\natural})</tex-math></inline-formula>, where <inline-formula id="S6.p2.m9"><tex-math>g</tex-math></inline-formula> can be any convex continuous function.</p><statement id="LABEL:cor_lasso_error"><title>.</title><p><italic>Consider the Gaussian linear model and the constrained LS estimator described above.
Assume that the entries of <inline-formula id="S6.Thmthm1.p1.m1"><tex-math>a_{1},\ldots,a_{n}</tex-math></inline-formula> are either all i.i.d. standard Gaussian or all i.i.d. Rademacher random variables.
Let <inline-formula id="S6.Thmthm1.p1.m2"><tex-math>\epsilon\in(0,1)</tex-math></inline-formula>. For any <inline-formula id="S6.Thmthm1.p1.m3"><tex-math>t\geq 0</tex-math></inline-formula>, there exist positive constants <inline-formula id="S6.Thmthm1.p1.m4"><tex-math>c_{1}</tex-math></inline-formula> and <inline-formula id="S6.Thmthm1.p1.m5"><tex-math>c_{2}</tex-math></inline-formula> such that if</italic></p><p><disp-formula id="LABEL:eq_lasso_sample_complexity"><tex-math>\sqrt{n}\geq\frac{c_{1}\alpha^{2}\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{%
\natural})\setminus t\mathcal{B}})}{\epsilon},</tex-math></disp-formula></p><p><italic>then we have</italic></p><p><disp-formula id="LABEL:eq_lasso_error"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq t+2\sqrt{2%
\pi}\sigma\,\frac{\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{\natural})%
\setminus t\mathcal{B}})}{(1-\epsilon)\sqrt{n}},</tex-math></disp-formula></p><p><italic>with probability at least <inline-formula id="S6.Thmthm1.p1.m6"><tex-math>1-\exp(-c_{2}\epsilon^{2}n)&gt;1/2</tex-math></inline-formula> when <inline-formula id="S6.Thmthm1.p1.m7"><tex-math>n</tex-math></inline-formula> is large enough.</italic></p></statement><statement id="Thmremx2"><title>.</title><p>When the constraint is matched, we can simply set <inline-formula id="Thmremx2.p1.m1"><tex-math>t=0</tex-math></inline-formula>.
Recall that <inline-formula id="Thmremx2.p1.m2"><tex-math>t</tex-math></inline-formula> cannot be zero for the mismatched constraint case when <inline-formula id="Thmremx2.p1.m3"><tex-math>n&lt;p</tex-math></inline-formula> (cf. Section <xref ref-type="labelref" rid="LABEL:sec_mismatch">5</xref>). This remark also applies to Corollary <xref ref-type="labelref" rid="LABEL:cor_glm_error">6.2</xref> below.</p></statement><statement id="Thmremx3"><title>.</title><p>For the mismatched constraint case, Corollary (<xref ref-type="labelref" rid="LABEL:cor_lasso_error">6.1</xref>) is minimax optimal for the Lasso in the Gaussian linear model. We address this in Section <xref ref-type="labelref" rid="LABEL:sec_mismatch_further">7</xref>.</p></statement><p>Corollary <xref ref-type="labelref" rid="LABEL:cor_lasso_error">6.1</xref> is consistent with <xref ref-type="bibr" rid="Oymak2013">[Oymak2013]</xref>. The result in <xref ref-type="bibr" rid="Oymak2013">[Oymak2013]</xref> is sharper, while Corollary <xref ref-type="labelref" rid="LABEL:cor_lasso_error">6.1</xref> is more general as it also covers the mismatched constraint case.</p><p>The second application is <inline-formula id="S6.p4.m1"><tex-math>\ell_{1}</tex-math></inline-formula>-constrained ML regression in a canonical GLM. </p><statement id="LABEL:cor_glm_error"><title>.</title><p><italic>Consider the canonical GLM and the constrained ML estimator described in Section <xref ref-type="labelref" rid="LABEL:sec_matched">4</xref>, for <inline-formula id="S6.Thmthm2.p1.m1"><tex-math>g(\theta):=\left\|\theta\right\|_{1}</tex-math></inline-formula> and <inline-formula id="S6.Thmthm2.p1.m2"><tex-math>c\geq\left\|\theta^{\natural}\right\|</tex-math></inline-formula>. Assume that <inline-formula id="S6.Thmthm2.p1.m3"><tex-math>f_{n}</tex-math></inline-formula> in (<xref ref-type="labelref" rid="LABEL:eq_fn_glm">13</xref>) is twice continuously differentiable, and the entries of <inline-formula id="S6.Thmthm2.p1.m4"><tex-math>a_{1},\ldots,a_{n}</tex-math></inline-formula> are i.i.d. Rademacher random variables. Let <inline-formula id="S6.Thmthm2.p1.m5"><tex-math>\epsilon\in(0,1)</tex-math></inline-formula>. For any <inline-formula id="S6.Thmthm2.p1.m6"><tex-math>t\geq 0</tex-math></inline-formula>, there exist positive constants <inline-formula id="S6.Thmthm2.p1.m7"><tex-math>c_{1}</tex-math></inline-formula>, and <inline-formula id="S6.Thmthm2.p1.m8"><tex-math>c_{2}</tex-math></inline-formula> such that if (<xref ref-type="labelref" rid="LABEL:eq_lasso_sample_complexity">15</xref>) is satisfied, then we have</italic></p><p><disp-formula id="LABEL:eq_glm_error"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq t+2\sqrt{2%
\pi}\,\sigma_{\max}\frac{\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{\natural%
})\setminus t\mathcal{B}})}{(1-\epsilon)\sqrt{n}},</tex-math></disp-formula></p><p><italic>with probability at least <inline-formula id="S6.Thmthm2.p1.m9"><tex-math>1-\exp(c_{2}\epsilon^{2}n)&gt;1/2</tex-math></inline-formula> when <inline-formula id="S6.Thmthm2.p1.m10"><tex-math>n</tex-math></inline-formula> is large enough, where <inline-formula id="S6.Thmthm2.p1.m11"><tex-math>\sigma_{\max}:=\max_{i}\sqrt{\mathrm{var}\,y_{i}}</tex-math></inline-formula> is bounded above by a constant independent of <inline-formula id="S6.Thmthm2.p1.m12"><tex-math>n</tex-math></inline-formula>.</italic></p></statement><p>To the best of our knowledge, there are not existing results for <inline-formula id="S6.p5.m1"><tex-math>\ell_{1}</tex-math></inline-formula>-constrained ML regression in GLMs. Here we compare Corollary <xref ref-type="labelref" rid="LABEL:cor_glm_error">6.2</xref> with <xref ref-type="bibr" rid="Negahban2010">[Negahban2010]</xref>, which provides an error bound for <inline-formula id="S6.p5.m2"><tex-math>\ell_{1}</tex-math></inline-formula>-penalized ML estimators in GLMs
. Recall that, however, the correspondence between the constrained and penalized estimators is currently unclear. When the constraint is matched and <inline-formula id="S6.p5.m3"><tex-math>\theta^{\natural}</tex-math></inline-formula> is <inline-formula id="S6.p5.m4"><tex-math>s</tex-math></inline-formula>-sparse, Corollary <xref ref-type="labelref" rid="LABEL:cor_glm_error">6.2</xref> states that when <inline-formula id="S6.p5.m5"><tex-math>n=\Omega(s\log(p/s))</tex-math></inline-formula>,</p><p><disp-formula id="S6.Ex19"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}=O\left(\sqrt{%
\frac{s}{n}\log\left(\frac{p}{s}\right)}\right)</tex-math></disp-formula></p><p>by Proposition 3.10 in <xref ref-type="bibr" rid="Chandrasekaran2012">[Chandrasekaran2012]</xref>, which essentially coincides with Corollary 5 in <xref ref-type="bibr" rid="Negahban2010">[Negahban2010]</xref><fn id="idp4491488"><p>We cite <xref ref-type="bibr" rid="Negahban2010">[Negahban2010]</xref> instead of the published version <xref ref-type="bibr" rid="Negahban2012">[Negahban2012]</xref>, because the estimation error bound only appears in <xref ref-type="bibr" rid="Negahban2010">[Negahban2010]</xref>.</p></fn>. We note that <xref ref-type="bibr" rid="Negahban2010">[Negahban2010]</xref> only provides an error bound for the <inline-formula id="S6.p5.m6"><tex-math>\ell_{1}</tex-math></inline-formula>-penalization case.
</p></sec><sec id="LABEL:sec_mismatch_further"><title>Sharpness of Our Error Bound</title><p>It has been shown that in a Gaussian linear model with <inline-formula id="S7.p1.m1"><tex-math>\mathcal{G}</tex-math></inline-formula> being an <inline-formula id="S7.p1.m2"><tex-math>\ell_{1}</tex-math></inline-formula>-ball, <italic>any</italic> estimator <inline-formula id="S7.p1.m3"><tex-math>\hat{\theta}_{\text{arbitrary}}</tex-math></inline-formula> must satisfy, with probability larger than <inline-formula id="S7.p1.m4"><tex-math>1/2</tex-math></inline-formula>,</p><p><disp-formula id="S7.Ex20"><tex-math>\max_{\theta^{\natural}\in\mathcal{G}}\left\|\hat{\theta}_{\text{arbitrary}}-%
\theta^{\natural}\right\|_{2}=\Omega(n^{-1/4}),</tex-math></disp-formula></p><p>under some technical conditions <xref ref-type="bibr" rid="Raskutti2011">[Raskutti2011]</xref>. Now we show our error bound for the Lasso in Corollary <xref ref-type="labelref" rid="LABEL:cor_lasso_error">6.1</xref> actually achieves the error decaying rate <inline-formula id="S7.p1.m5"><tex-math>O(n^{-1/4})</tex-math></inline-formula> in the mismatched constraint case, and hence cannot be essentially improved.</p><p>By the definition of the Gaussian width, we have, for any <inline-formula id="S7.p2.m1"><tex-math>t&gt;0</tex-math></inline-formula>,
</p><p><disp-formula id="S7.Ex21"><tex-math>\omega_{1}\left(\overline{\mathcal{F}_{g}(\theta^{\natural})\setminus t%
\mathcal{B}}\right)=\frac{\omega_{t}\left(\overline{\mathcal{F}_{g}(\theta^{%
\natural})\setminus t\mathcal{B}}\right)}{t}=\frac{\omega_{t}\left(\mathcal{F}%
_{g}(\theta^{\natural})\right)}{t},</tex-math></disp-formula></p><p>and hence the estimation error bound in Corollary <xref ref-type="labelref" rid="LABEL:eq_lasso_error">16</xref> can be written as</p><p><disp-formula id="LABEL:compare"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq t+\frac{C}{t%
}\frac{\omega_{t}(\mathcal{F}_{g}(\theta^{\natural}))}{\sqrt{n}},</tex-math></disp-formula></p><p>for some <inline-formula id="S7.p2.m2"><tex-math>C&gt;0</tex-math></inline-formula>, when <inline-formula id="S7.p2.m3"><tex-math>n</tex-math></inline-formula> is large enough such that (<xref ref-type="labelref" rid="LABEL:eq_lasso_sample_complexity">15</xref>) is satisfied.</p><p>Define the <italic>global Gaussian width</italic>:</p><p><disp-formula id="S7.Ex22"><tex-math>\omega(\mathcal{F}_{g}(\theta^{\natural})):=\mathbb{E}\,\sup_{v\in\mathcal{F}_%
{g}(\theta^{\natural})}\left\{\left\langle h,x\right\rangle\right\},</tex-math></disp-formula></p><p>where <inline-formula id="S7.p3.m1"><tex-math>h\in\mathbb{R}^{p}</tex-math></inline-formula> is a vector of i.i.d. standard Gaussian random variables. By definition, <inline-formula id="S7.p3.m2"><tex-math>\omega_{t}(\mathcal{F}_{g}(\theta^{\natural}))</tex-math></inline-formula> is bounded above by <inline-formula id="S7.p3.m3"><tex-math>\omega(\mathcal{F}_{g}(\theta^{\natural}))</tex-math></inline-formula>, independent of <inline-formula id="S7.p3.m4"><tex-math>n</tex-math></inline-formula>. Replacing <inline-formula id="S7.p3.m5"><tex-math>\omega_{t}(\mathcal{F}_{g}(\theta^{\natural}))</tex-math></inline-formula> by <inline-formula id="S7.p3.m6"><tex-math>\omega(\mathcal{F}_{g}(\theta^{\natural}))</tex-math></inline-formula> in (<xref ref-type="labelref" rid="LABEL:compare">18</xref>), we have a looser error upper bound:</p><p><disp-formula id="S7.Ex23"><tex-math>\mathbb{E}\,\left\|\hat{\theta}-\theta^{\natural}\right\|_{2}\leq t+\frac{C}{t%
}\frac{\omega(\mathcal{F}_{g}(\theta^{\natural}))}{\sqrt{n}},</tex-math></disp-formula></p><p>Minimizing this bound over all <inline-formula id="S7.p3.m7"><tex-math>t&gt;0</tex-math></inline-formula>, we obtain the <inline-formula id="S7.p3.m8"><tex-math>O(n^{-1/4})</tex-math></inline-formula> error decaying rate. Similar discussion can be found in <xref ref-type="bibr" rid="Plan2014a">[Plan2014a]</xref>.</p></sec><sec id="S8"><title>Discussion</title><p>Note that by the elementary argument in Section <xref ref-type="labelref" rid="LABEL:sec_framework">3</xref>, we arrive at an estimation error bound (<xref ref-type="labelref" rid="LABEL:eq_concentration">12</xref>) that holds <italic>surely</italic>. It is possible to derive a concentration-type error guarantee based on this sure error bound, which we are working on.</p><p>Our framework is not restricted to constraint sets of the form (<xref ref-type="labelref" rid="LABEL:eq_that">1</xref>); it applies to any non-empty closed convex set <inline-formula id="S8.p2.m1"><tex-math>\mathcal{G}</tex-math></inline-formula>, as we only require <inline-formula id="S8.p2.m2"><tex-math>\iota_{\mathcal{G}}(\cdot)</tex-math></inline-formula> to be proper closed convex in the proof. This observation is crucial to applying our framework to analyze constrained estimators for quantum tomography <xref ref-type="bibr" rid="Flammia2012">[Flammia2012, Gross2010]</xref> and photon-limited imaging systems <xref ref-type="bibr" rid="Raginsky2010">[Raginsky2010]</xref>, which we are studying.</p><p>In this paper, we consider a random matrix <inline-formula id="S8.p3.m1"><tex-math>A</tex-math></inline-formula>, and discuss the expected estimation error with respect to both <inline-formula id="S8.p3.m2"><tex-math>A</tex-math></inline-formula> and the sample <inline-formula id="S8.p3.m3"><tex-math>(y_{1},\ldots,y_{n})</tex-math></inline-formula>. The extension to the the case where <inline-formula id="S8.p3.m4"><tex-math>A</tex-math></inline-formula> is deterministic is technically non-trivial, and we have not obtained a satisfactory result. We address this in the remark following the proof of Theorem <xref ref-type="labelref" rid="LABEL:thm_no_mismatch">4.1</xref> in the appendix.</p></sec><sec id="Sx1"><title>Acknowledgements</title><p>This work was supported in part by the European Commission under Grant MIRG-268398, ERC Future Proof, SNF 200021-132548, SNF 200021-146750 and SNF CRSII2-147633.</p></sec></body>
  <back>
	An error in the conversion from LaTeX to XML has occurred here. 
<ref-list><title>References</title></ref-list><app-group><app id="A1"><title>Proof of Proposition 3.1</title><p>We have</p><disp-formula-group id="A5.EGx3"><disp-formula id="A1.Ex24"><tex-math/></disp-formula></disp-formula-group><p>The right-hand side is always larger than <inline-formula id="A1.p1.m1"><tex-math>\mu\left\|e\right\|_{2}^{2}</tex-math></inline-formula> by assumption.</p></app><app id="A2"><title>Proof of Theorem 4.1</title><p>The main goal of the proof is to evaluate <inline-formula id="A2.p1.m1"><tex-math>\mathbb{E}\,\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}\left(-%
\nabla f_{n}(\theta^{\natural})\right)\right\|_{2}</tex-math></inline-formula>. Here the expectation is with respect to both <inline-formula id="A2.p1.m2"><tex-math>A</tex-math></inline-formula> and the sample <inline-formula id="A2.p1.m3"><tex-math>(y_{i})_{i=1,\ldots,n}</tex-math></inline-formula>.</p><p>We start with an equivalent formulation:</p><p><disp-formula id="LABEL:eq_proj_sup"><tex-math>\mathbb{E}\,\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}\left(-%
\nabla f_{n}(\theta^{\natural})\right)\right\|_{2}=\mathbb{E}\,\sup_{v\in%
\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap\mathcal{S}^{p-1}}\left\{%
\left\langle-\nabla f_{n}(\theta^{\natural}),v\right\rangle\right\},</tex-math></disp-formula></p><p>where <inline-formula id="A2.p2.m1"><tex-math>\mathcal{S}^{p-1}</tex-math></inline-formula> denotes the unit <inline-formula id="A2.p2.m2"><tex-math>\ell_{2}</tex-math></inline-formula>-sphere in <inline-formula id="A2.p2.m3"><tex-math>\mathbb{R}^{p}</tex-math></inline-formula>. It is well known that in a canonical GLM, we have</p><p><disp-formula id="LABEL:eq_nablaF"><tex-math>\nabla f_{n}(\theta^{\natural})=-\frac{1}{n}A^{T}\varepsilon,</tex-math></disp-formula></p><p>where <inline-formula id="A2.p2.m4"><tex-math>\varepsilon:=(y_{i}-\mathbb{E}\,y_{i})_{i=1,\ldots,n}</tex-math></inline-formula>, and hence</p><p><disp-formula id="A2.Ex25"><tex-math>\mathbb{E}\,\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}\left(-%
\nabla f_{n}(\theta^{\natural})\right)\right\|_{2}=\frac{1}{n}\,\mathbb{E}\,%
\sup_{v\in\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap\mathcal{S}^{p-1}}%
\left\{\left\langle A^{T}\varepsilon,v\right\rangle\right\}.</tex-math></disp-formula></p><p>To proceed, we need the following symmetrization inequality. The symmetrization inequality is different from the well-known symmetrization inequality by a Rademacher process, so we show it here for completeness.</p><statement id="LABEL:lem_symmetrization"><title> (<xref ref-type="bibr" rid="Handel2014">[Handel2014]</xref>).</title><p><italic>Let <inline-formula id="A2.Thmthm1.p1.m1"><tex-math>\xi_{1},\ldots,\xi_{n}</tex-math></inline-formula> be independent real-valued random variables, and let <inline-formula id="A2.Thmthm1.p1.m2"><tex-math>\mathcal{F}</tex-math></inline-formula> be a class of real functions. We have</italic></p><p><disp-formula id="A2.Ex26"><tex-math>\mathbb{E}\,\sup_{f\in\mathcal{F}}\left\{\sum_{i=1}^{n}\left[f(\xi_{i})-%
\mathbb{E}\,f(\xi_{i})\right]\right\}\leq\sqrt{2\pi}\,\mathbb{E}\,\sup_{f\in%
\mathcal{F}}\left\{\sum_{i=1}^{n}h_{i}f(\xi_{i})\right\},</tex-math></disp-formula></p><p><italic>where <inline-formula id="A2.Thmthm1.p1.m3"><tex-math>h_{1},\ldots,h_{n}</tex-math></inline-formula> are i.i.d. standard Gaussian random variables.</italic></p></statement><statement id="Thmremx4"><title>.</title><p>In <xref ref-type="bibr" rid="Handel2014">[Handel2014]</xref>, the lemma is stated for the case when <inline-formula id="Thmremx4.p1.m1"><tex-math>\xi_{1},\ldots,\xi_{n}</tex-math></inline-formula> are i.i.d. The case when <inline-formula id="Thmremx4.p1.m2"><tex-math>\xi_{1},\ldots,\xi_{n}</tex-math></inline-formula> are not necessarily identical can be proved in a similar way, as noted in <xref ref-type="bibr" rid="Pollard1984">[Pollard1984]</xref>.</p></statement><p>By Lemma <xref ref-type="labelref" rid="LABEL:lem_symmetrization">B.1</xref>, we have</p><disp-formula-group id="A5.EGx4"><disp-formula id="A2.Ex27"><tex-math/></disp-formula><disp-formula id="A2.Ex28"><tex-math/></disp-formula></disp-formula-group><p>where <inline-formula id="A2.p4.m1"><tex-math>h\cdot\varepsilon:=(h_{i}\varepsilon_{i})_{i=1,\ldots,n}</tex-math></inline-formula>, and <inline-formula id="A2.p4.m2"><tex-math>h_{1},\ldots,h_{n}</tex-math></inline-formula> are i.i.d. standard Gaussian random variables. Note that <inline-formula id="A2.p4.m3"><tex-math>h\cdot\varepsilon</tex-math></inline-formula> is a random Gaussian vector with zero mean and covariance matrix <inline-formula id="A2.p4.m4"><tex-math>\Sigma\in\mathbb{R}^{n\times n}</tex-math></inline-formula> which is dependent on <inline-formula id="A2.p4.m5"><tex-math>A</tex-math></inline-formula> in general; moreover, since the entries in <inline-formula id="A2.p4.m6"><tex-math>\varepsilon</tex-math></inline-formula> are independent, <inline-formula id="A2.p4.m7"><tex-math>\Sigma</tex-math></inline-formula> is a diagonal matrix with diagonal entries given by <inline-formula id="A2.p4.m8"><tex-math>\Sigma_{i,i}:=\mathrm{var}\,y_{i}</tex-math></inline-formula>. Define <inline-formula id="A2.p4.m9"><tex-math>\tilde{h}:=(\tilde{h}_{i})_{i=1,\ldots,n}</tex-math></inline-formula>, where <inline-formula id="A2.p4.m10"><tex-math>\tilde{h}_{i}:=\Sigma_{i,i}^{-1/2}h_{i}\varepsilon_{i}</tex-math></inline-formula>. Then <inline-formula id="A2.p4.m11"><tex-math>\tilde{h}</tex-math></inline-formula> is a vector of i.i.d. standard Gaussian random variables; furthermore, it is still a vector of i.i.d. standard Gaussian random variables condition on <inline-formula id="A2.p4.m12"><tex-math>A</tex-math></inline-formula>, and hence it is statistically independent of <inline-formula id="A2.p4.m13"><tex-math>A</tex-math></inline-formula>.</p><p>Since <inline-formula id="A2.p5.m1"><tex-math>h\cdot\varepsilon</tex-math></inline-formula> and <inline-formula id="A2.p5.m2"><tex-math>\sqrt{\Sigma}\tilde{h}</tex-math></inline-formula> have the same probability distribution, we can write
</p><p><disp-formula id="A2.Ex29"><tex-math>\mathbb{E}\,\sup_{v\in\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap%
\mathcal{S}^{p-1}}\left\{\left\langle h\cdot\varepsilon,Av\right\rangle\right%
\}=\mathbb{E}\,\sup_{v\in\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap%
\mathcal{S}^{p-1}}\left\{\left\langle\sqrt{\Sigma}\tilde{h},Av\right\rangle%
\right\}.</tex-math></disp-formula></p><p>Let <inline-formula id="A2.p5.m3"><tex-math>\mathcal{T}:=\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap\mathcal{S}^{p-1}</tex-math></inline-formula>. Condition on any given <inline-formula id="A2.p5.m4"><tex-math>A</tex-math></inline-formula> (and hence <inline-formula id="A2.p5.m5"><tex-math>\Sigma</tex-math></inline-formula>), we consider two mean-zero Gaussian processes <inline-formula id="A2.p5.m6"><tex-math>\left\{X_{t}\right\}_{t\in\mathcal{T}}</tex-math></inline-formula> and <inline-formula id="A2.p5.m7"><tex-math>\left\{Y_{t}\right\}_{t\in\mathcal{T}}</tex-math></inline-formula> defined as</p><p><disp-formula id="A2.Ex30"><tex-math>X_{t}:=\left\langle\sqrt{\Sigma}\tilde{h},At\right\rangle,\quad Y_{t}:=\sigma_%
{\max}\left\langle\tilde{h},At\right\rangle,</tex-math></disp-formula></p><p>where <inline-formula id="A2.p5.m8"><tex-math>\sigma_{max}:=\max_{i}\Sigma_{i,i}=\max_{i}\sqrt{\mathrm{var}\,\varepsilon_{i}}</tex-math></inline-formula>. We have, for any <inline-formula id="A2.p5.m9"><tex-math>t_{1},t_{2}\in\mathcal{T}</tex-math></inline-formula>,</p><p><disp-formula id="A2.Ex31"><tex-math>\mathbb{E}\,\left|X_{t_{1}}-X_{t_{2}}\right|^{2}=\left\|\Sigma A(t_{1}-t_{2})%
\right\|_{2}^{2}\leq\sigma_{\max}^{2}\left\|A(t_{1}-t_{2})\right\|_{2}^{2}=%
\mathbb{E}\,\left|Y_{t_{1}}-Y_{t_{2}}\right|^{2}.</tex-math></disp-formula></p><p>By Slepian’s lemma, this implies</p><p><disp-formula id="A2.Ex32"><tex-math>\mathbb{E}\,\sup_{t\in\mathcal{T}}X_{t}\leq\mathbb{E}\,\sup_{t\in\mathcal{T}}Y%
_{t}.</tex-math></disp-formula></p><p>Since the inequality holds given any realization of <inline-formula id="A2.p5.m10"><tex-math>A</tex-math></inline-formula>, we have</p><disp-formula-group id="A5.EGx5"><disp-formula id="A2.Ex33"><tex-math/></disp-formula><disp-formula id="A2.Ex34"><tex-math/></disp-formula></disp-formula-group><p>It remains to prove</p><p><disp-formula id="LABEL:eq_key"><tex-math>\mathbb{E}\,\sup_{v\in\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap%
\mathcal{S}^{p-1}}\left\{\left\langle A^{T}\tilde{h},v\right\rangle\right\}%
\leq\sqrt{n}\,\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{\natural})}):=\sqrt%
{n}\,\mathbb{E}\,\sup_{v\in\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap%
\mathcal{S}^{p-1}}\left\{\left\langle\tilde{h},v\right\rangle\right\}.</tex-math></disp-formula></p><p>We consider two cases:</p><boxed-text id="A2.SS3.SSS0.Px1"><caption><p>Case 1: </p></caption><p>If <inline-formula id="A2.SS3.SSS0.Px1.p1.m1"><tex-math>A</tex-math></inline-formula> has i.i.d. standard Gaussian entries, then condition on <inline-formula id="A2.SS3.SSS0.Px1.p1.m2"><tex-math>\tilde{h}</tex-math></inline-formula>, <inline-formula id="A2.SS3.SSS0.Px1.p1.m3"><tex-math>A^{T}\tilde{h}</tex-math></inline-formula> is a vector of mean-zero Gaussian random variables with covariance matrix <inline-formula id="A2.SS3.SSS0.Px1.p1.m4"><tex-math>\left\|\tilde{h}\right\|_{2}I</tex-math></inline-formula>, and hence has the same probablity distribution as <inline-formula id="A2.SS3.SSS0.Px1.p1.m5"><tex-math>\left\|\tilde{h}\right\|\bar{h}</tex-math></inline-formula>, where <inline-formula id="A2.SS3.SSS0.Px1.p1.m6"><tex-math>\bar{h}</tex-math></inline-formula> is a vector of i.i.d. standard Gaussian random variables independent of <inline-formula id="A2.SS3.SSS0.Px1.p1.m7"><tex-math>\tilde{h}</tex-math></inline-formula>. Therefore,</p><disp-formula-group id="A5.EGx6"><disp-formula id="A2.Ex35"><tex-math/></disp-formula><disp-formula id="A2.Ex36"><tex-math/></disp-formula><disp-formula id="A2.Ex37"><tex-math/></disp-formula></disp-formula-group></boxed-text><boxed-text id="A2.SS3.SSS0.Px2"><caption><p>Case 2: </p></caption><p>If <inline-formula id="A2.SS3.SSS0.Px2.p1.m1"><tex-math>A</tex-math></inline-formula> has i.i.d. Rademacher entries, then condition on <inline-formula id="A2.SS3.SSS0.Px2.p1.m2"><tex-math>A</tex-math></inline-formula>, <inline-formula id="A2.SS3.SSS0.Px2.p1.m3"><tex-math>A^{T}\tilde{h}</tex-math></inline-formula> is a vector of mean-zero Gaussian random variables with covariance matrix <inline-formula id="A2.SS3.SSS0.Px2.p1.m4"><tex-math>nI</tex-math></inline-formula>, and hence has the same probability distribution as <inline-formula id="A2.SS3.SSS0.Px2.p1.m5"><tex-math>\sqrt{n}\bar{h}</tex-math></inline-formula>, where <inline-formula id="A2.SS3.SSS0.Px2.p1.m6"><tex-math>\bar{h}</tex-math></inline-formula> is a vector of i.i.d. standard Gaussian random variables. Therefore,</p><disp-formula-group id="A5.EGx7"><disp-formula id="A2.Ex38"><tex-math/></disp-formula><disp-formula id="A2.Ex39"><tex-math/></disp-formula></disp-formula-group><p>In summary, we obtain</p><p><disp-formula id="A2.Ex40"><tex-math>\mathbb{E}\,\left\|\Pi_{\overline{\mathcal{F}_{g}(\theta^{\natural})}}(-\nabla
f%
_{n}(\theta^{\natural}))\right\|_{2}\leq\sqrt{2\pi}\,\sigma_{\max}\,\frac{%
\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{\natural})})}{\sqrt{n}},</tex-math></disp-formula></p><p>if the entries of <inline-formula id="A2.SS3.SSS0.Px2.p2.m1"><tex-math>A</tex-math></inline-formula> are i.i.d. standard Gaussian or Rademacher random variables, for a canonical GLM, where the expectation is with respect to both <inline-formula id="A2.SS3.SSS0.Px2.p2.m2"><tex-math>A</tex-math></inline-formula> and the sample <inline-formula id="A2.SS3.SSS0.Px2.p2.m3"><tex-math>(y_{i})_{i=1,\ldots,n}</tex-math></inline-formula>.</p><p>Let <inline-formula id="A2.SS3.SSS0.Px2.p3.m1"><tex-math>\mathcal{E}</tex-math></inline-formula> denote that event that the RSC condition holds. Then we have</p><disp-formula-group id="A5.EGx8"><disp-formula id="A2.Ex41"><tex-math/></disp-formula><disp-formula id="A2.Ex42"><tex-math/></disp-formula></disp-formula-group><p>and hence</p><disp-formula-group id="A5.EGx9"><disp-formula id="A2.Ex43"><tex-math/></disp-formula><disp-formula id="A2.Ex44"><tex-math/></disp-formula></disp-formula-group><p>where we applied the assumption that <inline-formula id="A2.SS3.SSS0.Px2.p3.m2"><tex-math>\mathbb{P}(\mathcal{E})\geq 1/2</tex-math></inline-formula>. By Lemma 3.2, this implies</p><disp-formula-group id="A5.EGx10"><disp-formula id="A2.Ex45"><tex-math/></disp-formula><disp-formula id="A2.Ex46"><tex-math/></disp-formula></disp-formula-group><p>This completes the proof.</p><statement id="Thmremx5"><title>.</title><p>If we want to adapt this proof to the deterministic <inline-formula id="Thmremx5.p1.m1"><tex-math>A</tex-math></inline-formula> case, a technical issue arises when bounding the right-hand side of (<xref ref-type="labelref" rid="LABEL:eq_key">21</xref>). As the random process <inline-formula id="Thmremx5.p1.m2"><tex-math>\left\{\tilde{X}_{v}:=\left\langle\tilde{h},v\right\rangle\right\}_{v\in%
\mathcal{V}}</tex-math></inline-formula>, where <inline-formula id="Thmremx5.p1.m3"><tex-math>\mathcal{V}:=\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap\mathcal{S}^{p-1}</tex-math></inline-formula>, is a mean-zero Gaussian process, a standard approach is to bound <inline-formula id="Thmremx5.p1.m4"><tex-math>\sup_{v\in\mathcal{V}}\tilde{X}_{v}</tex-math></inline-formula> by Slepian’s lemma. Note that, for any <inline-formula id="Thmremx5.p1.m5"><tex-math>v_{1},v_{2}\in\mathcal{V}</tex-math></inline-formula>,</p><p><disp-formula id="A2.Ex47"><tex-math>\mathbb{E}\,\left|\tilde{X}_{v_{1}}-\tilde{X}_{v_{2}}\right|^{2}=\left\|A(v_{1%
}-v_{2})\right\|_{2}^{2},</tex-math></disp-formula></p><p>and hence an upper-bound on <inline-formula id="Thmremx5.p1.m6"><tex-math>\mathbb{E}\,\left|\tilde{X}_{v_{1}}-\tilde{X}_{v_{2}}\right|^{2}</tex-math></inline-formula> would depend on the largest eigenvalue of <inline-formula id="Thmremx5.p1.m7"><tex-math>A</tex-math></inline-formula>. The largest eigenvalue of <inline-formula id="Thmremx5.p1.m8"><tex-math>A</tex-math></inline-formula>, however, cannot be bounded above by a constant independent of <inline-formula id="Thmremx5.p1.m9"><tex-math>n</tex-math></inline-formula> under the high-dimensional setting. Although we can weaken the requirement on <inline-formula id="Thmremx5.p1.m10"><tex-math>A</tex-math></inline-formula> to a restricted smoothness condition as</p><p><disp-formula id="A2.Ex48"><tex-math>\left\|Av\right\|_{2}\leq\sqrt{1+\epsilon}\left\|v\right\|_{2},\quad\text{for %
all }v\in\overline{\mathcal{F}_{g}(\theta^{\natural})}\cap\mathcal{S}^{p-1},</tex-math></disp-formula></p><p>which, by Theorem <xref ref-type="labelref" rid="LABEL:thm_mendelson">E.1</xref>, holds with high probability. This condition does not imply
</p><p><disp-formula id="A2.Ex49"><tex-math>\left\|A(v_{1}-v_{2})\right\|_{2}^{2}\leq C\left\|v_{1}-v_{2}\right\|_{2}^{2},</tex-math></disp-formula></p><p>for some dimension-independent constant <inline-formula id="Thmremx5.p1.m11"><tex-math>C&gt;0</tex-math></inline-formula>, for all <inline-formula id="Thmremx5.p1.m12"><tex-math>v_{1},v_{2}\in\mathcal{V}</tex-math></inline-formula>.</p></statement></boxed-text></app><app id="LABEL:sec_proof_lem_mismatch"><title>Proof of Lemma 5.1</title><p>Let <inline-formula id="A3.p1.m1"><tex-math>e:=\hat{\theta}-\theta^{\natural}</tex-math></inline-formula>. If <inline-formula id="A3.p1.m2"><tex-math>e\in\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}</tex-math></inline-formula>, following the proof of Theorem 4.1, we obtain</p><p><disp-formula id="A3.Ex50"><tex-math>\left\|e\right\|_{2}\leq\frac{1}{\mu}\left\|\Pi_{\overline{\mathcal{F}_{g}(%
\theta^{\natural})\setminus t\mathcal{B}}}\left(-\nabla f_{n}(\theta^{\natural%
})\right)\right\|_{2},</tex-math></disp-formula></p><p>where <inline-formula id="A3.p1.m3"><tex-math>\overline{\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}}</tex-math></inline-formula> denotes the conic hull of <inline-formula id="A3.p1.m4"><tex-math>\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}</tex-math></inline-formula>. If <inline-formula id="A3.p1.m5"><tex-math>e\in t\mathcal{B}</tex-math></inline-formula>, we have the naïve bound: <inline-formula id="A3.p1.m6"><tex-math>\left\|e\right\|_{2}\leq t</tex-math></inline-formula>. Therefore,</p><disp-formula-group id="A5.EGx11"><disp-formula id="A3.Ex51"><tex-math/></disp-formula><disp-formula id="A3.Ex52"><tex-math/></disp-formula></disp-formula-group><p>The lemma follows by taking expectations on both sides.</p></app><app id="LABEL:sec_proof_cor_mismatch"><title>Proof of Corollary 5.2</title><p>Let <inline-formula id="A4.p1.m1"><tex-math>e:=\hat{\theta}-\theta^{\natural}</tex-math></inline-formula>. If <inline-formula id="A4.p1.m2"><tex-math>e\in\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}</tex-math></inline-formula>, following the proof of Theorem 4.1, we can obtain</p><p><disp-formula id="A4.Ex53"><tex-math>\mathbb{E}\,\left\|e\right\|_{2}\leq 2\sqrt{2\pi}\,\sigma_{\max}\frac{\omega_{%
1}(\overline{\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}})}{\mu%
\sqrt{n}};</tex-math></disp-formula></p><p>otherwise, we can bound the expected estimation error from above by <inline-formula id="A4.p1.m3"><tex-math>t</tex-math></inline-formula>. Therefore,</p><disp-formula-group id="A5.EGx12"><disp-formula id="A4.Ex54"><tex-math/></disp-formula><disp-formula id="A4.Ex55"><tex-math/></disp-formula></disp-formula-group></app><app id="A5"><title>Proof of Corollary 6.1 and Corollary 6.2</title><p>The proofs in this section rely on the following theorem <xref ref-type="bibr" rid="Mendelson2007">[Mendelson2007]</xref>.</p><statement id="LABEL:thm_mendelson"><title> (<xref ref-type="bibr" rid="Mendelson2007">[Mendelson2007]</xref>).</title><p><italic>Let <inline-formula id="A5.Thmthm1.p1.m1"><tex-math>\mathcal{T}\subseteq\mathbb{R}^{p}</tex-math></inline-formula> be star-shaped. Let <inline-formula id="A5.Thmthm1.p1.m2"><tex-math>A\in\mathbb{R}^{n\times p}</tex-math></inline-formula>, <inline-formula id="A5.Thmthm1.p1.m3"><tex-math>n&lt;p</tex-math></inline-formula>, whose rows are i.i.d. isotropic subgaussian random vectors with subgaussian norm <inline-formula id="A5.Thmthm1.p1.m4"><tex-math>\alpha\geq 1</tex-math></inline-formula>, and let <inline-formula id="A5.Thmthm1.p1.m5"><tex-math>\epsilon\in(0,1)</tex-math></inline-formula>. Then there exist constants <inline-formula id="A5.Thmthm1.p1.m6"><tex-math>c_{1}</tex-math></inline-formula> and <inline-formula id="A5.Thmthm1.p1.m7"><tex-math>c_{2}</tex-math></inline-formula> such that for all <inline-formula id="A5.Thmthm1.p1.m8"><tex-math>x\in\mathcal{T}</tex-math></inline-formula> satisfying</italic></p><p><disp-formula id="LABEL:eq_rkstar"><tex-math>\left\|x\right\|_{2}\geq\gamma_{n}^{*}\left(\frac{\epsilon}{c_{1}\alpha^{2}},%
\mathcal{T}\right):=\inf\left\{t&gt;0:t\geq\frac{c_{1}\alpha^{2}\omega_{t}(%
\mathcal{T})}{\epsilon\sqrt{n}}\right\},</tex-math></disp-formula></p><p><italic>we have</italic></p><p><disp-formula id="A5.Ex56"><tex-math>(1-\epsilon)\left\|x\right\|_{2}^{2}\leq\frac{\left\|Ax\right\|_{2}^{2}}{n}%
\leq(1+\epsilon)\left\|x\right\|_{2}^{2}</tex-math></disp-formula></p><p><italic>with probability at least <inline-formula id="A5.Thmthm1.p1.m9"><tex-math>1-\exp\left(-c_{2}\epsilon^{2}n/\alpha^{4}\right)</tex-math></inline-formula>.</italic></p></statement><p>We note that the sub-Gaussian norm of a vector of i.i.d. standard Gaussian entries or i.i.d. Rademacher entries is bounded above by a constant <xref ref-type="bibr" rid="Vershynin2012">[Vershynin2012]</xref>.</p><sec id="LABEL:sec_Lasso"><title>Proof of Corollary 6.1</title><p>We prove by Corollary 5.2.</p><p>Let <inline-formula id="A5.SS1.p2.m1"><tex-math>A</tex-math></inline-formula> be defined as in Theorem 4.1. We verify the condition (14) by Theorem <xref ref-type="labelref" rid="LABEL:thm_mendelson">E.1</xref>. Since <inline-formula id="A5.SS1.p2.m2"><tex-math>\omega_{t}(\overline{\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{B}}%
)=t\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{\natural})\setminus t\mathcal{%
B}})</tex-math></inline-formula>, the condition (<xref ref-type="labelref" rid="LABEL:eq_rkstar">22</xref>) is equivalent to requiring</p><p><disp-formula id="A5.Ex57"><tex-math>\sqrt{n}\geq\frac{c_{1}\alpha^{2}\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{%
\natural})\setminus t\mathcal{B}})}{\epsilon}.</tex-math></disp-formula></p><p>Once this inequality is satisfied, we can set <inline-formula id="A5.SS1.p2.m3"><tex-math>\mu=1-\epsilon</tex-math></inline-formula>, and the condition (14) hold with probability at least <inline-formula id="A5.SS1.p2.m4"><tex-math>1-\exp\left(-c_{2}\epsilon^{2}n/\alpha^{4}\right)</tex-math></inline-formula>. Note that <inline-formula id="A5.SS1.p2.m5"><tex-math>\sigma_{\max}=\sqrt{\mathbb{E}\,w_{i}^{2}}=\sigma</tex-math></inline-formula>. This completes the proof.</p></sec><sec id="A5.SS2"><title>Proof of Corollary 6.2</title><p>We prove the corollary by Corollary 5.2.</p><p>It is known that</p><p><disp-formula id="A5.Ex58"><tex-math>\nabla^{2}f_{n}(\theta)=\frac{1}{n}A^{T}D(\theta)A</tex-math></disp-formula></p><p>for the ML estimator in a canonical GLM, where <inline-formula id="A5.SS2.p2.m1"><tex-math>A</tex-math></inline-formula> is defined as in Theorem 4.1, and <inline-formula id="A5.SS2.p2.m2"><tex-math>D(\theta)</tex-math></inline-formula> is a diagonal matrix; furthermore, there exists a continuous strictly positive function <inline-formula id="A5.SS2.p2.m3"><tex-math>\phi</tex-math></inline-formula> such that the <inline-formula id="A5.SS2.p2.m4"><tex-math>(i,i)</tex-math></inline-formula>-th entry of <inline-formula id="A5.SS2.p2.m5"><tex-math>D(\theta)</tex-math></inline-formula> is given by <inline-formula id="A5.SS2.p2.m6"><tex-math>\phi(\langle a_{i},\theta\rangle)</tex-math></inline-formula>. Since the entries of <inline-formula id="A5.SS2.p2.m7"><tex-math>A</tex-math></inline-formula> are i.i.d. Rademacher random variables, for any <inline-formula id="A5.SS2.p2.m8"><tex-math>\theta\in\mathcal{G}</tex-math></inline-formula>,</p><p><disp-formula id="A5.Ex59"><tex-math>\left|\left\langle a_{i},\theta\right\rangle\right|\leq\left\|a_{i}\right\|_{%
\infty}\left\|\theta\right\|_{1}\leq c.</tex-math></disp-formula></p><p>By the extreme value theorem, the diagonal entries of <inline-formula id="A5.SS2.p2.m9"><tex-math>D(\theta)</tex-math></inline-formula> are bounded below by a constant <inline-formula id="A5.SS2.p2.m10"><tex-math>\nu&gt;0</tex-math></inline-formula> for all <inline-formula id="A5.SS2.p2.m11"><tex-math>\theta\in\mathcal{G}</tex-math></inline-formula>, which is independent of <inline-formula id="A5.SS2.p2.m12"><tex-math>n</tex-math></inline-formula>. Similarly, <inline-formula id="A5.SS2.p2.m13"><tex-math>\sigma_{\max}</tex-math></inline-formula> is bounded above by a constant independent of <inline-formula id="A5.SS2.p2.m14"><tex-math>n</tex-math></inline-formula>.</p><p>The rest of the proof is similar to the last paragraph in the previous sub-section.
By Theorem <xref ref-type="labelref" rid="LABEL:thm_mendelson">E.1</xref>, if we choose <inline-formula id="A5.SS2.p3.m1"><tex-math>n</tex-math></inline-formula> such that</p><p><disp-formula id="A5.Ex60"><tex-math>\sqrt{n}\geq\frac{c\alpha^{2}\omega_{1}(\overline{\mathcal{F}_{g}(\theta^{%
\natural})\setminus t\mathcal{B}})}{\epsilon},</tex-math></disp-formula></p><p>then the condition (14) holds with probability at least <inline-formula id="A5.SS2.p3.m2"><tex-math>1-\exp\left(-c_{2}\epsilon^{2}n/\alpha^{4}\right)</tex-math></inline-formula> with <inline-formula id="A5.SS2.p3.m3"><tex-math>\mu=\nu(1-\epsilon)</tex-math></inline-formula>.</p></sec></app></app-group></back>
</article>
